# Tools

[Tools](https://python.langchain.com/docs/concepts/tools/) are a way to encapsulate a function and its input schema in a way that can be passed to a chat model that supports tool calling. This allows the model to request the execution of this function with specific inputs.

## Create tools

You can pass a vanilla function to `create_react_agent` to use as a tool:

```python
def multiply(a: int, b: int) -> int:
   """Multiply two numbers."""
   return a * b

create_react_agent(
    model="anthropic:claude-3-7-sonnet",
    tools=[multiply]
)
```

`create_react_agent` automatically converts vanilla functions to [LangChain tools](https://python.langchain.com/docs/concepts/tools/#tool-interface).

## Customize tools

If you want more control over how the tool is created, you can use `@tool` decorator:

```python
# highlight-next-line
from langchain_core.tools import tool

# highlight-next-line
@tool("multiply_tool", parse_docstring=True)
def multiply(a: int, b: int) -> int:
    """Multiply two numbers.

    Args:
        a: First operand
        b: Second operand
    """
    return a * b
```

A common pattern is defining a custom tool input schema as a Pydantic model:

```python
from pydantic import BaseModel, Field

class MultiplyInputSchema(BaseModel):
    """Multiply two numbers"""
    a: int = Field(description="First operand")
    b: int = Field(description="Second operand")

# highlight-next-line
@tool("multiply_tool", args_schema=MultiplyInputSchema)
def multiply(a: int, b: int) -> int:
   return a * b
```

For even more control and creating custom LangChain tools, see [this guide](https://python.langchain.com/docs/how_to/custom_tools/).

## Hide arguments from the model

There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. In LangGraph's `create_react_agent`, you might want to pass agent [state](./context.md#via-state-tools) or [config](./context.md#via-config-tools) to the tools.

```python
from langgraph.prebuilt import InjectedState
from langgraph.prebuilt.chat_agent_executor import AgentState
from langchain_core.runnables import RunnableConfig

def my_tool(
    # This will be populated by an LLM
    tool_arg: str,
    # access information that's dynamically updated inside the agent
    # highlight-next-line
    state: Annotated[AgentState, InjectedState],
    # access static data that is passed at agent invocation
    # highlight-next-line
    config: RunnableConfig,
) -> str:
    """My tool."""
    do_something_with_state(state["messages"])
    do_something_with_config(config)
    ...
```

## Parallel tool calling

Some model providers allow their models to request execution of several tools at once and allow users to explicitly enable or disable parallel tool calling. You can control this in `create_react_agent` via passing a `ChatModel` instance with bound tools:

```python
from langchain.chat_models import init_chat_model

def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

def multiply(a: int, b: int) -> int:
   """Multiply two numbers."""
   return a * b

model = init_chat_model("anthropic:claude-3-5-sonnet-latest", temperature=0)
tools = [add, multiply]
agent = create_react_agent(
    # disable parallel tool calls
    # highlight-next-line
    model=model.bind_tools(tools, parallel_tool_calls=False),
    tools=tools
)

agent.invoke({"messages": "what's 3 + 5 and 4 * 7? make both calculations in parallel"})
```

## Return tool results directly

`create_react_agent` allows you to return tool response directly and end the tool-calling loop early. To do so, you must specify `return_direct=True` when creating your tool:

```python
from langchain_core.tools import tool

# highlight-next-line
@tool(return_direct=True)
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[add]
)

agent.invoke({"messages": "what's 3 + 5?"})
```

## Force tool use

Similar to parallel tool calling, certain providers allow users to specify `tool_choice`, indicating if the agent is always required to call a certain tools (or any tool). You can similarly control this via `model.bind_tools()`.

!!! Warning

    If you set `tool_choice` to always call a particular tool, you will end up with an infinite tool-calling loop, as the stopping condition for `create_react_agent` is the absence of tool calls requested by an LLM. To address this you have a few options:

    - mark your tool as `return_direct=True`. This will stop the agent loop after the the tool is executed for the first time (used in example below). 
    - specify `recursion_limit` when invoking the agent. This will limit the maximum number of graph steps that the agent can take before hitting an GraphRecursionError.

```python
from langchain_core.tools import tool

# highlight-next-line
@tool(return_direct=True)
def greet(user_name: str) -> int:
    """Greet user."""
    return f"Hello {user_name}!"

tools = [greet]

agent = create_react_agent(
    # highlight-next-line
    model=model.bind_tools(tools, tool_choice={"type": "tool", "name": "greet"}),
    tools=tools
)

agent.invoke({"messages": "Hi, I am Bob"})
```