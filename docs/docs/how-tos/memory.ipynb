{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ed7413-c876-4d38-b080-79c71133549b",
   "metadata": {},
   "source": [
    "# Manage memory\n",
    "\n",
    "Many AI applications need memory to share context across multiple interactions. LangGraph supports two types of memory essential for building conversational agents:\n",
    "\n",
    "- [Short-term memory](#add-short-term-memory): Tracks the ongoing conversation by maintaining message history within a session.\n",
    "- [Long-term memory](#add-long-term-memory): Stores user-specific or application-level data across sessions.\n",
    "\n",
    "With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:\n",
    "\n",
    "* [Trimming](#trim-messages): Remove first or last N messages (before calling LLM)\n",
    "* [Summarization](#summarize-messages): Summarize earlier messages in the history and replace them with a summary\n",
    "* [Delete messages](#delete-messages) from LangGraph state permanently\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "This allows the agent to keep track of the conversation without exceeding the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph \"langchain[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38b03c-5609-49e3-9b93-bad0aab47ffb",
   "metadata": {},
   "source": [
    "## Add short-term memory\n",
    "\n",
    "Short-term memory enables agents to track multi-turn conversations:\n",
    "\n",
    "```python\n",
    "# highlight-next-line\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# highlight-next-line\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "builder = StateGraph(...)\n",
    "# highlight-next-line\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n",
    "    # highlight-next-line\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "```\n",
    "\n",
    "See the [persistence](../persistence#add-short-term-memory) guide to learn more about working with short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf55fd-b0b0-4fbf-9f15-aefac7f300bb",
   "metadata": {},
   "source": [
    "## Add long-term memory\n",
    "\n",
    "Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.\n",
    "\n",
    "```python\n",
    "# highlight-next-line\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# highlight-next-line\n",
    "store = InMemoryStore()\n",
    "\n",
    "builder = StateGraph(...)\n",
    "# highlight-next-line\n",
    "graph = builder.compile(store=store)\n",
    "```\n",
    "\n",
    "See the [persistence](../persistence#add-long-term-memory) guide to learn more about working with long-term memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f30aeefe-f5fb-496a-a9d8-aaf785db481c",
   "metadata": {},
   "source": [
    "## Process model input\n",
    "\n",
    "When managing long conversation history, you will need to preprocess the list of messages before sending it to the LLM (e.g., [trim](#trim-messages), [summarize](#summarize-messages) or [delete](#delete-messages) messages).\n",
    "\n",
    "### Handle cheap computation\n",
    "\n",
    "If the preprocessing is cheap and doesn't need to be persisted or streamed separately, you can add it directly to the model-calling graph node:\n",
    "\n",
    "```python\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```\n",
    "\n",
    "??? example \"Full example: process messages in a model-calling node\"\n",
    "\n",
    "    ```python\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    \n",
    "    from langgraph.graph import MessagesState, StateGraph, START\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    \n",
    "    def process_messages(messages: list[AnyMessage]):\n",
    "        # keep only the last message\n",
    "        return messages[-1:]\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        # highlight-next-line\n",
    "        messages = process_messages(state[\"messages\"])\n",
    "        response = model.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    \n",
    "    # highight-next-line\n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    # highight-next-line\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the process_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    hi! I'm bob\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    Hi Bob! How are you doing today? Is there anything I can help you with?\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    what's my name?\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    I apologize, but you haven't told me your name yet. Could you please introduce yourself?\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fb083e7-698a-4809-844f-5361013e6323",
   "metadata": {},
   "source": [
    "### Handle expensive computation\n",
    "\n",
    "If the preprocessing step involves **expensive computation** (e.g., message [summarization](#summarize-messages)) and needs to be persisted or streamed separately, add the input processing as a separate graph node:\n",
    "\n",
    "```python\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "        \"some_key\": \"some_value\"  # (2)!\n",
    "    }\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ed02d-858c-40e1-8387-d53940c08ac5",
   "metadata": {},
   "source": [
    "1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "2. `some_key` will be added to the **overall** graph state.\n",
    "3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6181647a-9e36-49b4-9eaa-e133e1550252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "??? example \"Full example: process messages in a separate node\"\n",
    "\n",
    "    ```python\n",
    "    from typing_extensions import TypedDict\n",
    "    \n",
    "    class OverallState(MessagesState):\n",
    "        some_key: str\n",
    "    \n",
    "    \n",
    "    class LLMInputState(TypedDict):\n",
    "        # highlight-next-line\n",
    "        llm_input_messages: list[AnyMessage]\n",
    "    \n",
    "    \n",
    "    def process_messages(state: MessagesState):\n",
    "        return {\n",
    "            # keep only the last message\n",
    "            \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "            \"some_key\": \"some_value\"  # (2)!\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"llm_input_messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(OverallState)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(process_messages)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"process_messages\")\n",
    "    builder.add_edge(\"process_messages\", \"call_model\")\n",
    "    \n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the filter_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    ```\n",
    "    \n",
    "    1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "    2. `some_key` will be added to the **overall** graph state.\n",
    "    3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state.\n",
    "\n",
    "    ```\n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
    "\n",
    "\n",
    "    {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    \n",
    "    \n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
    "    \n",
    "    \n",
    "    {'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109c1b2-a44e-4ec0-8a11-1377e59315c6",
   "metadata": {},
   "source": [
    "## Trim messages\n",
    "\n",
    "TODO: populate this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6676cd03-3e97-4550-ad4a-72d04f2cee7c",
   "metadata": {},
   "source": [
    "## Summarize messages\n",
    "\n",
    "An effective strategy for handling long conversation history is to summarize earlier messages once they reach a certain threshold:\n",
    "\n",
    "```python\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "# highlight-next-line\n",
    "from langmem.short_term import SummarizationNode\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]  # (1)!\n",
    "\n",
    "class LLMInputState(TypedDict):  # (2)!\n",
    "    summarized_messages: list[AnyMessage]\n",
    "    context: dict[str, Any]\n",
    "\n",
    "# highlight-next-line\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=512,\n",
    "    max_tokens_before_summary=256,\n",
    "    max_summary_tokens=256,\n",
    ")\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"summarized_messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "# highlight-next-line\n",
    "builder.add_node(\"summarize\", summarization_node)\n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "...\n",
    "```\n",
    "\n",
    "1. We will keep track of our running summary in the `context` field\n",
    "(expected by the `SummarizationNode`).\n",
    "2. Define private state that will be used only for filtering\n",
    "the inputs to `call_model` node.\n",
    "3. We're passing a private input state here to isolate the messages returned by the summarization node\n",
    "\n",
    "??? example \"Full example: summarize messages\"\n",
    "\n",
    "    ```python\n",
    "    from typing import Any, TypedDict\n",
    "    \n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    from langchain_core.messages.utils import count_tokens_approximately\n",
    "    from langgraph.graph import StateGraph, START, MessagesState\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    # highlight-next-line\n",
    "    from langmem.short_term import SummarizationNode\n",
    "    \n",
    "    model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n",
    "    summarization_model = model.bind(max_tokens=128)\n",
    "    \n",
    "    class State(MessagesState):\n",
    "        # highlight-next-line\n",
    "        context: dict[str, Any]  # (1)!\n",
    "    \n",
    "    class LLMInputState(TypedDict):  # (2)!\n",
    "        summarized_messages: list[AnyMessage]\n",
    "        context: dict[str, Any]\n",
    "    \n",
    "    # highlight-next-line\n",
    "    summarization_node = SummarizationNode(\n",
    "        token_counter=count_tokens_approximately,\n",
    "        model=summarization_model,\n",
    "        max_tokens=256,\n",
    "        max_tokens_before_summary=256,\n",
    "        max_summary_tokens=128,\n",
    "    )\n",
    "\n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"summarized_messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(call_model)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(\"summarize\", summarization_node)\n",
    "    builder.add_edge(START, \"summarize\")\n",
    "    builder.add_edge(\"summarize\", \"call_model\")\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    # Invoke the graph\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "    graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "    graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "    final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "    final_response[\"messages\"][-1].pretty_print()\n",
    "    print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n",
    "    ```\n",
    "\n",
    "    1. We will keep track of our running summary in the `context` field\n",
    "    (expected by the `SummarizationNode`).\n",
    "    2. Define private state that will be used only for filtering\n",
    "    the inputs to `call_model` node.\n",
    "    3. We're passing a private input state here to isolate the messages returned by the summarization node\n",
    "\n",
    "    ```\n",
    "    ================================== Ai Message ==================================\n",
    "\n",
    "    From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n",
    "    \n",
    "    Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d880b-1258-4708-8f0e-5efc95031e78",
   "metadata": {},
   "source": [
    "## Delete messages\n",
    "\n",
    "To delete messages from the graph state, you can use the `RemoveMessage`.\n",
    "\n",
    "* Remove specific messages:\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    ```\n",
    "\n",
    "* Remove **all** messages:\n",
    "    \n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        # highlight-next-line\n",
    "        return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}\n",
    "    ```\n",
    "\n",
    "!!! important \"`add_messages` reducer\"\n",
    "\n",
    "    For `RemoveMessage` to work, you need to use a state key with [`add_messages`][langgraph.graph.message.add_messages] [reducer](../../../concepts/low_level#reducers), like [`MessagesState`](../../../concepts/low_level#messagesstate)\n",
    "\n",
    "!!! warning \"Valid message history\"\n",
    "\n",
    "    When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n",
    "    \n",
    "    * some providers expect message history to start with a `user` message\n",
    "    * most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n",
    "\n",
    "??? example \"Full example: delete messages\"\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_sequence([call_model, delete_messages])\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    [('human', \"hi! I'm bob\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
