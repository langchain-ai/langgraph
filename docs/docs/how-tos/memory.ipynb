{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ed7413-c876-4d38-b080-79c71133549b",
   "metadata": {},
   "source": [
    "# Manage Conversation History\n",
    "\n",
    "Long conversations can exceed the LLM's context window. Common solutions are:\n",
    "\n",
    "* [Trimming](#trim-messages) — Remove first or last N messages (before calling LLM)\n",
    "* [Summarization](#summarize-messages) — Summarize earlier messages in the history and replace them with a summary\n",
    "* [Deleting messages](#delete-messages) - Delete messages from LangGraph state permanently\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "This allows the agent to keep track of the conversation without exceeding the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph \"langchain[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ANTHROPIC_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c338d33-c17f-40a5-8167-a9ee4aaeeea7",
   "metadata": {},
   "source": [
    "## Process Model Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f6b34-33cf-4d19-afce-ab1a3dee7cd0",
   "metadata": {},
   "source": [
    "### In a model-calling node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959f0c83-0042-46ce-a797-7b671185695c",
   "metadata": {},
   "source": [
    "Use this if you need to process input to the LLM. In most cases, this will be sufficient.\n",
    "\n",
    "```python\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```\n",
    "\n",
    "??? example \"Full example: process messages in a model-calling node\"\n",
    "\n",
    "    ```python\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    \n",
    "    from langgraph.graph import MessagesState, StateGraph, START\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    \n",
    "    def process_messages(messages: list[AnyMessage]):\n",
    "        # keep only the last message\n",
    "        return messages[-1:]\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        # highlight-next-line\n",
    "        messages = process_messages(state[\"messages\"])\n",
    "        response = model.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    \n",
    "    # highight-next-line\n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    # highight-next-line\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the process_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    hi! I'm bob\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    Hi Bob! How are you doing today? Is there anything I can help you with?\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    what's my name?\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    I apologize, but you haven't told me your name yet. Could you please introduce yourself?\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5345fa93-4181-4eec-8dc9-0969969fa1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How are you doing today? Is there anything I can help you with?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize, but you haven't told me your name yet. Could you please introduce yourself?\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# highight-next-line\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "# highight-next-line\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the process_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae813e-0fbb-4454-bfde-1238d6496db5",
   "metadata": {},
   "source": [
    "### In a separate node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524da7-f877-4a1a-949e-3f3c4ec9991e",
   "metadata": {},
   "source": [
    "Use this if you need to:\n",
    "\n",
    "* stream and checkpoint message processing as a separate graph step\n",
    "* **both** preprocess inputs to the LLM **and** update the graph state. This is useful when summarizing message history, where `process_messages` step needs to return both the LLM input messages and a running summary information.\n",
    "\n",
    "```python\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "        \"some_key\": \"some_value\"  # (2)!\n",
    "    }\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ed02d-858c-40e1-8387-d53940c08ac5",
   "metadata": {},
   "source": [
    "1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "2. `some_key` will be added to the **overall** graph state.\n",
    "3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6181647a-9e36-49b4-9eaa-e133e1550252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "??? example \"Full example: process messages in a separate node\"\n",
    "\n",
    "    ```python\n",
    "    from typing_extensions import TypedDict\n",
    "    \n",
    "    class OverallState(MessagesState):\n",
    "        some_key: str\n",
    "    \n",
    "    \n",
    "    class LLMInputState(TypedDict):\n",
    "        # highlight-next-line\n",
    "        llm_input_messages: list[AnyMessage]\n",
    "    \n",
    "    \n",
    "    def process_messages(state: MessagesState):\n",
    "        return {\n",
    "            # keep only the last message\n",
    "            \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "            \"some_key\": \"some_value\"  # (2)!\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"llm_input_messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(OverallState)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(process_messages)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"process_messages\")\n",
    "    builder.add_edge(\"process_messages\", \"call_model\")\n",
    "    \n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the filter_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    ```\n",
    "    \n",
    "    1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "    2. `some_key` will be added to the **overall** graph state.\n",
    "    3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state.\n",
    "\n",
    "    ```\n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
    "\n",
    "\n",
    "    {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    \n",
    "    \n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
    "    \n",
    "    \n",
    "    {'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78faa1f2-6ae5-406d-baa3-9f1e00e87d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n",
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],\n",
    "        \"some_key\": \"some_value\"\n",
    "    }\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"process_messages\")\n",
    "builder.add_edge(\"process_messages\", \"call_model\")\n",
    "\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the filter_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65737a-dcf9-4987-9de8-948afd6a820d",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109c1b2-a44e-4ec0-8a11-1377e59315c6",
   "metadata": {},
   "source": [
    "### Trim messages\n",
    "\n",
    "TODO: populate this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e1df9-da3f-42d2-ace0-6ae8e99be98b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "### Delete messages\n",
    "\n",
    "To delete messages from the graph state, you can use the `RemoveMessage`.\n",
    "\n",
    "* Remove specific messages:\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    ```\n",
    "\n",
    "* Remove **all** messages:\n",
    "    \n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        # highlight-next-line\n",
    "        return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}\n",
    "    ```\n",
    "\n",
    "!!! important \"`add_messages` reducer\"\n",
    "\n",
    "    For `RemoveMessage` to work, you need to use a state key with [`add_messages`][langgraph.graph.message.add_messages] [reducer](../../../concepts/low_level#reducers), like [`MessagesState`](../../../concepts/low_level#messagesstate)\n",
    "\n",
    "!!! warning \"Valid message history\"\n",
    "\n",
    "    When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n",
    "    \n",
    "    * some providers expect message history to start with a `user` message\n",
    "    * most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n",
    "\n",
    "??? example \"Full example: delete messages\"\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_sequence([call_model, delete_messages])\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    [('human', \"hi! I'm bob\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "004205c4-c28e-41d8-86eb-519a51f366aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
      "[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([call_model, delete_messages])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06054c",
   "metadata": {},
   "source": [
    "### Summarize messages\n",
    "\n",
    "An effective strategy for handling long conversation history is to summarize earlier messages once they reach a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4279-1ed3-419c-8669-fa96addb69f5",
   "metadata": {},
   "source": [
    "#### TODO: rewrite / simplify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "378899a9-3b9a-4748-95b6-eb00e0828677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# We will add a `summary` attribute (in addition to `messages` key,\n",
    "# which MessagesState already has)\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# We will use this model for both the conversation and the summarization\n",
    "model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\n",
    "\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    # If a summary exists, we add this in as a system message\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# We now define the logic for determining whether to end or summarize the conversation\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    # First, we summarize the conversation\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        # If a summary already exists, we use a different system prompt\n",
    "        # to summarize it than if one didn't\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    # We now need to delete messages that we no longer want to show up\n",
    "    # I will delete all but the last two messages, but you can change this\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the conversation node and the summarize node\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `conversation`.\n",
    "    # This means these are the edges taken after the `conversation` node is called.\n",
    "    \"conversation\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `summarize_conversation` to END.\n",
    "# This means that after `summarize_conversation` is called, we end.\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Finally, we compile it!\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc697132-8fa1-4bf5-9722-56a9859331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_update(update):\n",
    "    for k, v in update.items():\n",
    "        for m in v[\"messages\"]:\n",
    "            m.pretty_print()\n",
    "        if \"summary\" in v:\n",
    "            print(v[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob, as you told me at the beginning of our conversation.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like the celtics!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great, the Celtics are a fun team to follow! Basketball is an exciting sport. Do you have a favorite Celtics player or a favorite moment from a Celtics game you've watched? I'd be happy to discuss the team and the sport with you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the celtics!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760e219-a7fc-4d81-b4e8-1334c5afc510",
   "metadata": {},
   "source": [
    "We can see that so far no summarization has happened - this is because there are only six messages in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "935265a0-d511-475a-8a0d-b3c3cc5e42a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"hi! I'm bob\", id='6534853d-b8a7-44b9-837b-eb7abaf7ebf7'),\n",
       "  AIMessage(content=\"It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. How can I help you today?\", response_metadata={'id': 'msg_015wCFew2vwMQJcpUh2VZ5ah', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 30}}, id='run-0d33008b-1094-4f5e-94ce-293283fc3024-0'),\n",
       "  HumanMessage(content=\"what's my name?\", id='0a4f203a-b95a-42a9-b1c5-bb20f68b3251'),\n",
       "  AIMessage(content='Your name is Bob, as you told me at the beginning of our conversation.', response_metadata={'id': 'msg_01PLp8wg2xDsJbNR9uCtxcGz', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 50, 'output_tokens': 19}}, id='run-3815dd4d-ee0c-4fc2-9889-f6dd40325961-0'),\n",
       "  HumanMessage(content='i like the celtics!', id='ac128172-42d1-4390-b7cc-7bcb2d22ee48'),\n",
       "  AIMessage(content=\"That's great, the Celtics are a fun team to follow! Basketball is an exciting sport. Do you have a favorite Celtics player or a favorite moment from a Celtics game you've watched? I'd be happy to discuss the team and the sport with you.\", response_metadata={'id': 'msg_01CSg5avZEx6CKcZsSvSVXpr', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 78, 'output_tokens': 61}}, id='run-698faa28-0f72-495f-8ebe-e948664d2200-0')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40eddb-9a31-4410-a4c0-9762e2d89e56",
   "metadata": {},
   "source": [
    "Now let's send another message in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048805a4-3d97-4e76-ac45-8d80d4364c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like how much they win\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite?\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Here is a summary of our conversation so far:\n",
      "\n",
      "- You introduced yourself as Bob and said you like the Boston Celtics basketball team.\n",
      "- I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.\n",
      "- You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.\n",
      "- I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.\n",
      "- The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like how much they win\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b196367-6151-4982-9430-3db7373de06e",
   "metadata": {},
   "source": [
    "If we check the state now, we can see that we have a summary of the conversation, as well as the last two messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09ebb693-4738-4474-a095-6491def5c5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='i like how much they win', id='bb916ce7-534c-4d48-9f92-e269f9dc4859'),\n",
       "  AIMessage(content=\"That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite?\", response_metadata={'id': 'msg_01B7TMagaM8xBnYXLSMwUDAG', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 148, 'output_tokens': 82}}, id='run-c5aa9a8f-7983-4a7f-9c1e-0c0055334ac1-0')],\n",
       " 'summary': \"Here is a summary of our conversation so far:\\n\\n- You introduced yourself as Bob and said you like the Boston Celtics basketball team.\\n- I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.\\n- You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.\\n- I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.\\n- The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e4177-c0fc-4fd0-a494-dd03f7f2fddb",
   "metadata": {},
   "source": [
    "We can now resume having a conversation! Note that even though we only have the last two messages, we can still ask it questions about things mentioned earlier in the conversation (because we summarized those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7094c5ab-66f8-42ff-b1c3-90c8a9468e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In our conversation so far, you introduced yourself as Bob. I acknowledged that earlier when you had shared your name.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40e5db8e-9db9-4ac7-9d76-a99fd4034bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what NFL team do you think I like?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't actually have any information about what NFL team you might like. In our conversation so far, you've only mentioned that you're a fan of the Boston Celtics basketball team. I don't have any prior knowledge about your preferences for NFL teams. Unless you provide me with that information, I don't have a basis to guess which NFL team you might be a fan of.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"what NFL team do you think I like?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a1a0fda-5309-45f0-9465-9f3dff604d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like the patriots!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, got it! Thanks for sharing that you're also a fan of the New England Patriots in the NFL. That makes sense, given your interest in other Boston sports teams like the Celtics. The Patriots have also had a very successful run over the past couple of decades, winning multiple Super Bowls. It's fun to follow winning franchises like the Celtics and Patriots. Do you have a favorite Patriots player or moment that stands out to you?\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Okay, extending the summary with the new information:\n",
      "\n",
      "- You initially introduced yourself as Bob and said you like the Boston Celtics basketball team. \n",
      "- I acknowledged that and we discussed your appreciation for the Celtics' history of winning.\n",
      "- You then asked what your name was, and I reminded you that you had introduced yourself as Bob earlier in the conversation.\n",
      "- You followed up by asking what NFL team I thought you might like, and I explained that I didn't have any prior information about your NFL team preferences.\n",
      "- You then revealed that you are also a fan of the New England Patriots, which made sense given your Celtics fandom.\n",
      "- I responded positively to this new information, noting the Patriots' own impressive success and dynasty over the past couple of decades.\n",
      "- I then asked if you have a particular favorite Patriots player or moment that stands out to you, continuing the friendly, conversational tone.\n",
      "\n",
      "Overall, the discussion has focused on your sports team preferences, with you sharing that you are a fan of both the Celtics and the Patriots. I've tried to engage with your interests and ask follow-up questions to keep the dialogue flowing.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like the patriots!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
