{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ed7413-c876-4d38-b080-79c71133549b",
   "metadata": {},
   "source": [
    "# Manage memory\n",
    "\n",
    "## TODO: update this part to reflect that this is a general memory guide\n",
    "\n",
    "Long conversations can exceed the LLM's context window. Common solutions are:\n",
    "\n",
    "* [Trimming](#trim-messages) — Remove first or last N messages (before calling LLM)\n",
    "* [Summarization](#summarize-messages) — Summarize earlier messages in the history and replace them with a summary\n",
    "* [Deleting messages](#delete-messages) - Delete messages from LangGraph state permanently\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "This allows the agent to keep track of the conversation without exceeding the LLM's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph \"langchain[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f058e-062a-48fc-947e-9ae8074b98f0",
   "metadata": {},
   "source": [
    "## Add memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38b03c-5609-49e3-9b93-bad0aab47ffb",
   "metadata": {},
   "source": [
    "### Short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf55fd-b0b0-4fbf-9f15-aefac7f300bb",
   "metadata": {},
   "source": [
    "### Long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c338d33-c17f-40a5-8167-a9ee4aaeeea7",
   "metadata": {},
   "source": [
    "## Process model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f6b34-33cf-4d19-afce-ab1a3dee7cd0",
   "metadata": {},
   "source": [
    "### Handle cheap computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959f0c83-0042-46ce-a797-7b671185695c",
   "metadata": {},
   "source": [
    "Use this if you the input processing is cheap and doesn't need to be persisted or streamed separately.\n",
    "\n",
    "```python\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```\n",
    "\n",
    "??? example \"Full example: process messages in a model-calling node\"\n",
    "\n",
    "    ```python\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    \n",
    "    from langgraph.graph import MessagesState, StateGraph, START\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    \n",
    "    def process_messages(messages: list[AnyMessage]):\n",
    "        # keep only the last message\n",
    "        return messages[-1:]\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        # highlight-next-line\n",
    "        messages = process_messages(state[\"messages\"])\n",
    "        response = model.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    \n",
    "    # highight-next-line\n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    # highight-next-line\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the process_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    hi! I'm bob\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    Hi Bob! How are you doing today? Is there anything I can help you with?\n",
    "    ================================ Human Message =================================\n",
    "    \n",
    "    what's my name?\n",
    "    ================================== Ai Message ==================================\n",
    "    \n",
    "    I apologize, but you haven't told me your name yet. Could you please introduce yourself?\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5345fa93-4181-4eec-8dc9-0969969fa1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How are you doing today? Is there anything I can help you with?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize, but you haven't told me your name yet. Could you please introduce yourself?\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# highight-next-line\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "# highight-next-line\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the process_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae813e-0fbb-4454-bfde-1238d6496db5",
   "metadata": {},
   "source": [
    "### Handle expensive computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524da7-f877-4a1a-949e-3f3c4ec9991e",
   "metadata": {},
   "source": [
    "Use this if you need to:\n",
    "\n",
    "* **both** preprocess inputs to the LLM **and** update the graph state. This is useful for **expensive computation**, like message summarization, where `process_messages` step needs to return both the LLM input messages and a running summary information\n",
    "* stream and checkpoint input processing as a separate graph step\n",
    "\n",
    "```python\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "        \"some_key\": \"some_value\"  # (2)!\n",
    "    }\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ed02d-858c-40e1-8387-d53940c08ac5",
   "metadata": {},
   "source": [
    "1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "2. `some_key` will be added to the **overall** graph state.\n",
    "3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6181647a-9e36-49b4-9eaa-e133e1550252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "??? example \"Full example: process messages in a separate node\"\n",
    "\n",
    "    ```python\n",
    "    from typing_extensions import TypedDict\n",
    "    \n",
    "    class OverallState(MessagesState):\n",
    "        some_key: str\n",
    "    \n",
    "    \n",
    "    class LLMInputState(TypedDict):\n",
    "        # highlight-next-line\n",
    "        llm_input_messages: list[AnyMessage]\n",
    "    \n",
    "    \n",
    "    def process_messages(state: MessagesState):\n",
    "        return {\n",
    "            # keep only the last message\n",
    "            \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "            \"some_key\": \"some_value\"  # (2)!\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"llm_input_messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(OverallState)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(process_messages)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"process_messages\")\n",
    "    builder.add_edge(\"process_messages\", \"call_model\")\n",
    "    \n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the filter_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    ```\n",
    "    \n",
    "    1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "    2. `some_key` will be added to the **overall** graph state.\n",
    "    3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state.\n",
    "\n",
    "    ```\n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
    "\n",
    "\n",
    "    {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    \n",
    "    \n",
    "    {'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
    "    \n",
    "    \n",
    "    {'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78faa1f2-6ae5-406d-baa3-9f1e00e87d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n",
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],\n",
    "        \"some_key\": \"some_value\",\n",
    "    }\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"process_messages\")\n",
    "builder.add_edge(\"process_messages\", \"call_model\")\n",
    "\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the filter_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109c1b2-a44e-4ec0-8a11-1377e59315c6",
   "metadata": {},
   "source": [
    "## Trim messages\n",
    "\n",
    "TODO: populate this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e1df9-da3f-42d2-ace0-6ae8e99be98b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "## Delete messages\n",
    "\n",
    "To delete messages from the graph state, you can use the `RemoveMessage`.\n",
    "\n",
    "* Remove specific messages:\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    ```\n",
    "\n",
    "* Remove **all** messages:\n",
    "    \n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        # highlight-next-line\n",
    "        return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}\n",
    "    ```\n",
    "\n",
    "!!! important \"`add_messages` reducer\"\n",
    "\n",
    "    For `RemoveMessage` to work, you need to use a state key with [`add_messages`][langgraph.graph.message.add_messages] [reducer](../../../concepts/low_level#reducers), like [`MessagesState`](../../../concepts/low_level#messagesstate)\n",
    "\n",
    "!!! warning \"Valid message history\"\n",
    "\n",
    "    When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:\n",
    "    \n",
    "    * some providers expect message history to start with a `user` message\n",
    "    * most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.\n",
    "\n",
    "??? example \"Full example: delete messages\"\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langchain_core.messages import RemoveMessage\n",
    "    \n",
    "    def delete_messages(state):\n",
    "        messages = state[\"messages\"]\n",
    "        if len(messages) > 2:\n",
    "            # remove the earliest two messages\n",
    "            # highlight-next-line\n",
    "            return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_sequence([call_model, delete_messages])\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    \n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    [('human', \"hi! I'm bob\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
    "    [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "004205c4-c28e-41d8-86eb-519a51f366aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n",
      "[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([call_model, delete_messages])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6676cd03-3e97-4550-ad4a-72d04f2cee7c",
   "metadata": {},
   "source": [
    "## Summarize messages\n",
    "\n",
    "An effective strategy for handling long conversation history is to summarize earlier messages once they reach a certain threshold:\n",
    "\n",
    "```python\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "# highlight-next-line\n",
    "from langmem.short_term import SummarizationNode\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]  # (1)!\n",
    "\n",
    "class LLMInputState(TypedDict):  # (2)!\n",
    "    summarized_messages: list[AnyMessage]\n",
    "    context: dict[str, Any]\n",
    "\n",
    "# highlight-next-line\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=512,\n",
    "    max_tokens_before_summary=256,\n",
    "    max_summary_tokens=256,\n",
    ")\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"summarized_messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "# highlight-next-line\n",
    "builder.add_node(\"summarize\", summarization_node)\n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "...\n",
    "```\n",
    "\n",
    "1. We will keep track of our running summary in the `context` field\n",
    "(expected by the `SummarizationNode`).\n",
    "2. Define private state that will be used only for filtering\n",
    "the inputs to `call_model` node.\n",
    "3. We're passing a private input state here to isolate the messages returned by the summarization node\n",
    "\n",
    "??? example \"Full example: summarize messages\"\n",
    "\n",
    "    ```python\n",
    "    from typing import Any, TypedDict\n",
    "    \n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    from langchain_core.messages.utils import count_tokens_approximately\n",
    "    from langgraph.graph import StateGraph, START, MessagesState\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    # highlight-next-line\n",
    "    from langmem.short_term import SummarizationNode\n",
    "    \n",
    "    model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n",
    "    summarization_model = model.bind(max_tokens=128)\n",
    "    \n",
    "    class State(MessagesState):\n",
    "        # highlight-next-line\n",
    "        context: dict[str, Any]  # (1)!\n",
    "    \n",
    "    class LLMInputState(TypedDict):  # (2)!\n",
    "        summarized_messages: list[AnyMessage]\n",
    "        context: dict[str, Any]\n",
    "    \n",
    "    # highlight-next-line\n",
    "    summarization_node = SummarizationNode(\n",
    "        token_counter=count_tokens_approximately,\n",
    "        model=summarization_model,\n",
    "        max_tokens=256,\n",
    "        max_tokens_before_summary=256,\n",
    "        max_summary_tokens=128,\n",
    "    )\n",
    "\n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"summarized_messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    builder = StateGraph(State)\n",
    "    builder.add_node(call_model)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(\"summarize\", summarization_node)\n",
    "    builder.add_edge(START, \"summarize\")\n",
    "    builder.add_edge(\"summarize\", \"call_model\")\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    # Invoke the graph\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "    graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "    graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "    final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "    final_response[\"messages\"][-1].pretty_print()\n",
    "    print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n",
    "    ```\n",
    "\n",
    "    1. We will keep track of our running summary in the `context` field\n",
    "    (expected by the `SummarizationNode`).\n",
    "    2. Define private state that will be used only for filtering\n",
    "    the inputs to `call_model` node.\n",
    "    3. We're passing a private input state here to isolate the messages returned by the summarization node\n",
    "\n",
    "    ```\n",
    "    ================================== Ai Message ==================================\n",
    "\n",
    "    From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n",
    "    \n",
    "    Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "87e739d6-36d1-47c8-9a24-b799bf9043ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n",
      "\n",
      "Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langmem.short_term import SummarizationNode\n",
    "\n",
    "model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n",
    "summarization_model = model.bind(max_tokens=256)\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    # highlight-next-line\n",
    "    context: dict[str, Any]  # (1)!\n",
    "\n",
    "\n",
    "class LLMInputState(TypedDict):  # (2)!\n",
    "    summarized_messages: list[AnyMessage]\n",
    "    context: dict[str, Any]\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=512,\n",
    "    max_tokens_before_summary=256,\n",
    "    max_summary_tokens=256,\n",
    ")\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"summarized_messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "# highlight-next-line\n",
    "builder.add_node(\"summarize\", summarization_node)\n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is Bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
