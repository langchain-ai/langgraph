{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# Add persistence\n",
    "\n",
    "Many AI applications need memory to share context across multiple interactions. LangGraph supports two types of memory essential for building conversational agents:\n",
    "\n",
    "- **[Short-term memory](#add-short-term-memory)**: Tracks the ongoing conversation by maintaining message history within a session.\n",
    "- **[Long-term memory](#add-long-term-memory)**: Stores user-specific or application-level data across sessions.\n",
    "\n",
    "!!! note \"Terminology\"\n",
    "\n",
    "    In LangGraph:\n",
    "\n",
    "    - *Short-term memory* is also referred to as **thread-level memory**.\n",
    "    - *Long-term memory* is also called **cross-thread memory**.\n",
    "\n",
    "    A [thread](../../concepts/persistence#threads) represents a sequence of related runs\n",
    "    grouped by the same `thread_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph \"langchain[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ANTHROPIC_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9da1-9aaf-4a5f-9b1f-6ab1a273e6a9",
   "metadata": {},
   "source": [
    "## Add short-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c171e2-82d2-423f-8eba-ff32d7c494fe",
   "metadata": {},
   "source": [
    "To add **short-term** memory (thread-level persistence), we need to provide a [checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) when compiling the graph.\n",
    "\n",
    "Short-term memory enables agents to track multi-turn conversations. To use it, you must:\n",
    "\n",
    "1. Provide a `checkpointer` when creating the graph. The `checkpointer` enables [persistence](../concepts/persistence.md) of the agent's state.\n",
    "2. Supply a `thread_id` in the config when running the agent. The `thread_id` is a unique identifier for the conversation session.\n",
    "\n",
    "```python\n",
    "# highlight-next-line\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# highlight-next-line\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "builder = StateGraph(...)\n",
    "# highlight-next-line\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234c4c1-e8e2-45eb-91ea-7dc452c2fb2b",
   "metadata": {},
   "source": [
    "!!! info \"Not needed for LangGraph API users\"\n",
    "\n",
    "    If you're using the LangGraph API, **don't need** to provide checkpointer when compiling the graph. The API automatically handles checkpointing for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87326ea6-34c5-46da-a41f-dda26ef9bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "\n",
    "# highlight-next-line\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "# highlight-next-line\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How are you doing today? Is there anything I can help you with?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        # highlight-next-line\n",
    "        \"thread_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    # highlight-next-line\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    # highlight-next-line\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e067f-bcdc-4caa-b1f5-388387674d00",
   "metadata": {},
   "source": [
    "### Use a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a01e6-8962-4404-8100-5779edb666b4",
   "metadata": {},
   "source": [
    "In production, you would want to use a checkpointer backed by a database:\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "# highlight-next-line\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    builder = StateGraph(...)\n",
    "    # highlight-next-line\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "```\n",
    "\n",
    "??? example \"Example: using [Postgres](https://pypi.org/project/langgraph-checkpoint-postgres/) checkpointer\"\n",
    "\n",
    "    ```\n",
    "    pip install -U psycopg psycopg-pool langgraph langgraph-checkpoint-postgres\n",
    "    ```\n",
    "\n",
    "    !!! Setup\n",
    "        You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer\n",
    "\n",
    "    === \"Sync\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.postgres import PostgresSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "        # highlight-next-line\n",
    "        with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "            # checkpointer.setup()\n",
    "        \n",
    "            def call_model(state: MessagesState):\n",
    "                response = model.invoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "        ```\n",
    "\n",
    "    === \"Async\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "        # highlight-next-line\n",
    "        async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "            # await checkpointer.setup()\n",
    "        \n",
    "            async def call_model(state: MessagesState):\n",
    "                response = await model.ainvoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "        ```\n",
    "\n",
    "    \n",
    "\n",
    "??? example \"Example: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer\"\n",
    "\n",
    "    ```\n",
    "    pip install -U pymongo langgraph langgraph-checkpoint-mongodb\n",
    "    ```\n",
    "\n",
    "    !!! note \"Setup\"\n",
    "\n",
    "        To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.\n",
    "\n",
    "    === \"Sync\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"localhost:27017\"\n",
    "        # highlight-next-line\n",
    "        with MongoDBSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "        \n",
    "            def call_model(state: MessagesState):\n",
    "                response = model.invoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "        ```\n",
    "\n",
    "    === \"Async\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"localhost:27017\"\n",
    "        # highlight-next-line\n",
    "        async with AsyncMongoDBSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "        \n",
    "            async def call_model(state: MessagesState):\n",
    "                response = await model.ainvoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "        ```    \n",
    "\n",
    "??? example \"Example: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointer\"\n",
    "\n",
    "    ```\n",
    "    pip install -U langgraph langgraph-checkpoint-redis\n",
    "    ```\n",
    "\n",
    "    !!! Setup\n",
    "        You need to call `checkpointer.setup()` the first time you're using Redis checkpointer\n",
    "\n",
    "\n",
    "    === \"Sync\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.redis import RedisSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"redis://localhost:6379\"\n",
    "        # highlight-next-line\n",
    "        with RedisSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "            # checkpointer.setup()\n",
    "        \n",
    "            def call_model(state: MessagesState):\n",
    "                response = model.invoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            for chunk in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "        ```\n",
    "\n",
    "    === \"Async\"\n",
    "\n",
    "        ```python\n",
    "        from langchain.chat_models import init_chat_model\n",
    "        from langgraph.graph import StateGraph, MessagesState, START\n",
    "        # highlight-next-line\n",
    "        from langgraph.checkpoint.redis.aio import AsyncRedisSaver\n",
    "        \n",
    "        model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "        \n",
    "        DB_URI = \"redis://localhost:6379\"\n",
    "        # highlight-next-line\n",
    "        async with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "            # await checkpointer.asetup()\n",
    "        \n",
    "            async def call_model(state: MessagesState):\n",
    "                response = await model.ainvoke(state[\"messages\"])\n",
    "                return {\"messages\": response}\n",
    "        \n",
    "            builder = StateGraph(MessagesState)\n",
    "            builder.add_node(call_model)\n",
    "            builder.add_edge(START, \"call_model\")\n",
    "            \n",
    "            # highlight-next-line\n",
    "            graph = builder.compile(checkpointer=checkpointer)\n",
    "        \n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    # highlight-next-line\n",
    "                    \"thread_id\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()\n",
    "            \n",
    "            async for chunk in graph.astream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "                # highlight-next-line\n",
    "                config,\n",
    "                stream_mode=\"values\"\n",
    "            ):\n",
    "                chunk[\"messages\"][-1].pretty_print()     \n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f54d98-c659-49af-ae1e-a25641924ad1",
   "metadata": {},
   "source": [
    "### Use with [subgraphs](../../concepts/low_level#subgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ec9f4-08bc-4118-af7b-1d3c4a5ef69b",
   "metadata": {},
   "source": [
    "```python\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "# Subgraph\n",
    "\n",
    "def subgraph_node_1(state: State):\n",
    "    return {\"foo\": state[\"foo\"] + \"bar\"}\n",
    "\n",
    "subgraph_builder = StateGraph(State)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "# highlight-next-line\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "# Parent graph\n",
    "\n",
    "def node_1(state: State):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "# highlight-next-line\n",
    "builder.add_node(\"node_1\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "# highlight-next-line\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "```\n",
    "\n",
    "!!! important\n",
    "\n",
    "    You must only pass checkpointer when compiling the parent graph.\n",
    "    LangGraph will automatically propagate the checkpointer to the child subgraphs.\n",
    "\n",
    "If you want the subgraph to have its own memory, you can compile it `with checkpointer=True`. This is useful in [multi-agent](../../concepts/multi_agent) systems, if you want agents to keep track of their internal message histories:\n",
    "\n",
    "```python\n",
    "subgraph_builder = StateGraph(...)\n",
    "# highlight-next-line\n",
    "subgraph = subgraph_builder.compile(checkpointer=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b4493-c578-428f-857c-ef53f7139d53",
   "metadata": {},
   "source": [
    "### Use with [Functional API](../../concepts/functional_api)\n",
    "\n",
    "To add short-term memory to a Functional API LangGraph workflow:\n",
    "\n",
    "1. Pass `checkpointer` instance to the [`entrypoint()`][langgraph.func.entrypoint] decorator:\n",
    "\n",
    "    ```python\n",
    "    from langgraph.func import entrypoint\n",
    "    \n",
    "    @entrypoint(checkpointer=checkpointer)\n",
    "    def workflow(inputs)\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "2. Optionally expose `previous` parameter in the workflow function signature:\n",
    "\n",
    "    ```python\n",
    "    @entrypoint(checkpointer=checkpointer)\n",
    "    def workflow(\n",
    "        inputs,\n",
    "        *,\n",
    "        # you can optionally specify `previous` in the workflow function signature\n",
    "        # to access the return value from the workflow as of the last execution\n",
    "        previous\n",
    "    ):\n",
    "        previous = previous or []\n",
    "        combined_inputs = previous + inputs\n",
    "        result = do_something(combined_inputs)\n",
    "        ...\n",
    "    ```\n",
    "\n",
    "3. Optionally choose which values will be returned from the workflow and which will be saved by the checkpointer as `previous`:\n",
    "\n",
    "    ```python\n",
    "    @entrypoint(checkpointer=checkpointer)\n",
    "    def workflow(inputs, *, previous):\n",
    "        ...\n",
    "        result = do_something(...)\n",
    "        return entrypoint.final(value=result, save=combine(inputs, result))\n",
    "    ```\n",
    "\n",
    "??? example \"Example: add short-term memory to Functional API workflow\"\n",
    "\n",
    "    ```python\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    from langgraph.graph import add_messages\n",
    "    from langgraph.func import entrypoint, task\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "    # highlight-next-line\n",
    "    @task\n",
    "    def call_model(messages: list[AnyMessage]):\n",
    "        response = model.invoke(messages)\n",
    "        return response\n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "\n",
    "    # highlight-next-line\n",
    "    @entrypoint(checkpointer=checkpointer)\n",
    "    def workflow(inputs: list[AnyMessage], *, previous: list[AnyMessage]):\n",
    "        if previous:\n",
    "            inputs = add_messages(previous, inputs)\n",
    "    \n",
    "        response = call_model(inputs).result()\n",
    "        return entrypoint.final(value=response, save=add_messages(inputs, response))\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            # highlight-next-line\n",
    "            \"thread_id\": \"1\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for chunk in workflow.invoke(\n",
    "        [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk.pretty_print()\n",
    "    \n",
    "    for chunk in workflow.stream(\n",
    "        [{\"role\": \"user\", \"content\": \"what's my name?\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk.pretty_print()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799eaf41-1a48-4a31-bc71-1a2a51e92b21",
   "metadata": {},
   "source": [
    "## Add long-term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f3ae3-30e3-41d1-900a-c84879b50f86",
   "metadata": {},
   "source": [
    "Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.\n",
    "\n",
    "To use long-term memory, we need to [provide a store][langgraph.store.base.BaseStore] when creating the graph:\n",
    "\n",
    "```python\n",
    "# highlight-next-line\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# highlight-next-line\n",
    "store = InMemoryStore()\n",
    "\n",
    "builder = StateGraph(...)\n",
    "# highlight-next-line\n",
    "graph = builder.compile(store=store)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2222da-2177-4ae3-85cb-cbf143f44d93",
   "metadata": {},
   "source": [
    "!!! info \"Not needed for LangGraph API users\"\n",
    "\n",
    "    If you're using the LangGraph API, **don't need** to provide store when compiling the graph. The API automatically handles storage infrastructure for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88d8ede-ec8c-406c-917d-cda5610679c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "# highlight-next-line\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    # highlight-next-line\n",
    "    store: BaseStore   # (1)!\n",
    "):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "    # Store new memories if the user asks the model to remember\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        memory = \"User name is Bob\"\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "store = InMemoryStore()\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    # highlight-next-line\n",
    "    store=store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33d6ee-5754-4a9d-8246-6ad188a28d94",
   "metadata": {},
   "source": [
    "1. This is the `store` we compiled the graph with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ce2760-f23a-4ab1-b73c-d2680e20b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! Remember: my name is Bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! I'll remember that your name is Bob. How are you doing today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        # highlight-next-line\n",
    "        \"thread_id\": \"1\",\n",
    "        # highlight-next-line\n",
    "        \"user_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n",
    "    # highlight-next-line\n",
    "    config, \n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # highlight-next-line\n",
    "        \"thread_id\": \"2\",\n",
    "        \"user_id\": \"1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n",
    "    # highlight-next-line\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
