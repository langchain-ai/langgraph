{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5889ca0-6feb-4864-a630-e97ccc2c587e",
   "metadata": {},
   "source": [
    "# How to stream LLM tokens from specific nodes\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "A common use case when [streaming LLM tokens](../streaming-tokens) is to only stream them from specific nodes. To do so, you can use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "def node_a(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "def node_b(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(node_a)\n",
    "    .add_node(node_b)\n",
    "    ...\n",
    "    .compile()\n",
    "    \n",
    "for msg, metadata in graph.stream(\n",
    "    inputs,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # stream from 'node_a'\n",
    "    # highlight-next-line\n",
    "    if metadata[\"langgraph_node\"] == \"node_a\":\n",
    "        print(msg)\n",
    "```\n",
    "\n",
    "!!! note \"Streaming from a specific LLM invocation\"\n",
    "\n",
    "    If you need to instead filter streamed LLM tokens to a specific LLM invocation, check out [this guide](../streaming-tokens#filter-to-specific-llm-invocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff85bd-8a5d-409e-93d4-e9242b5e976d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05157237-783c-49de-9f29-7dca3c285647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd22cd2-3152-433b-ad50-65be8ace61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce8c26-f38d-4bdb-89ff-b058e7560019",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419a3c71-7bf6-4656-99b8-b5d61f3f4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    poem: str\n",
    "\n",
    "\n",
    "def write_joke(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    joke_response = model.invoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
    "    )\n",
    "    return {\"joke\": joke_response.content}\n",
    "\n",
    "\n",
    "def write_poem(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    poem_response = model.invoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
    "    )\n",
    "    return {\"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(write_joke)\n",
    "    .add_node(write_poem)\n",
    "    # write both the joke and the poem concurrently\n",
    "    .add_edge(START, \"write_joke\")\n",
    "    .add_edge(START, \"write_poem\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed84d5e-ba10-4324-a664-dca263951a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In| shadows| soft|,| they| quietly| creep|,|  \n",
      "|Wh|isk|ered| wonders|,| in| dreams| they| leap|.|  \n",
      "|With| eyes| like| lantern|s|,| bright| and| wide|,|  \n",
      "|Myst|eries| linger| where| they| reside|.|  \n",
      "\n",
      "|P|aws| that| pat|ter| on| silent| floors|,|  \n",
      "|Cur|led| in| sun|be|ams|,| they| seek| out| more|.|  \n",
      "|A| flick| of| a| tail|,| a| leap|,| a| p|ounce|,|  \n",
      "|In| their| playful| world|,| we| can't| help| but| bounce|.|  \n",
      "\n",
      "|Guard|ians| of| secrets|,| with| gentle| grace|,|  \n",
      "|Each| little| me|ow|,| a| warm| embrace|.|  \n",
      "|Oh|,| the| joy| that| they| bring|,| so| pure| and| true|,|  \n",
      "|In| the| heart| of| a| cat|,| there's| magic| anew|.|  |"
     ]
    }
   ],
   "source": [
    "for msg, metadata in graph.stream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
