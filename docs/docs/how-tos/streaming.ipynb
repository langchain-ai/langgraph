{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c4b04f-0c03-4321-9d40-38d12c59d088",
   "metadata": {},
   "source": [
    "# How to stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15403cdb-441d-43af-a29f-fc15abe03dcc",
   "metadata": {},
   "source": [
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
    "\n",
    "LangGraph is built with first class support for streaming. There are several different ways to stream back outputs from a graph run:\n",
    "\n",
    "- `\"values\"`: Emit all values in the state after each step.\n",
    "- `\"updates\"`: Emit only the node names and updates returned by the nodes after each step.\n",
    "    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n",
    "- `\"custom\"`: Emit custom data from inside nodes using `StreamWriter`.\n",
    "- [`\"messages\"`](../streaming-tokens): Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes.\n",
    "- `\"debug\"`: Emit debug events with as much information as possible for each step.\n",
    "\n",
    "You can stream outputs from the graph by using `graph.stream(..., stream_mode=<stream_mode>)` method, e.g.:\n",
    "\n",
    "=== \"Sync\"\n",
    "\n",
    "    ```python\n",
    "    for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "=== \"Async\"\n",
    "\n",
    "    ```python\n",
    "    async for chunk in graph.astream(inputs, stream_mode=\"updates\"):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "You can also combine multiple streaming mode by providing a list to `stream_mode` parameter:\n",
    "\n",
    "=== \"Sync\"\n",
    "\n",
    "    ```python\n",
    "    for chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "=== \"Async\"\n",
    "\n",
    "    ```python\n",
    "    async for chunk in graph.astream(inputs, stream_mode=[\"updates\", \"custom\"]):\n",
    "        print(chunk)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723cf76-6fe4-4b52-829f-3f28712ddcb7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427f8f66-7404-4c7d-a642-af5053b8b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03310ce6-e21f-4378-93bf-dd273fdb3e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80399508-bad8-43b7-8ec9-4c06ad1774cc",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4adbb2-61e8-4bb7-942d-b4dc27ba71ac",
   "metadata": {},
   "source": [
    "Let's define a simple graph with two nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4c513-1006-4179-bba9-d858fc952169",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faeb5ce8-d383-4277-b0a8-322e713638e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "\n",
    "\n",
    "def refine_topic(state: State):\n",
    "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
    "\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b90850-85bf-4391-b6b7-22ad45edaa3b",
   "metadata": {},
   "source": [
    "## Stream all values in the state (stream_mode=\"values\") {#values}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed60d4-cf78-4d4d-a660-6879539e168f",
   "metadata": {},
   "source": [
    "Use this to stream **all values** in the state after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daca06a-369b-41e5-8e4e-6edc4d4af3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'ice cream'}\n",
      "{'topic': 'ice cream and cats'}\n",
      "{'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb1bdb-f9fa-4d42-87ce-8e25d4290883",
   "metadata": {},
   "source": [
    "## Stream state updates from the nodes (stream_mode=\"updates\") {#updates}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c55326-d077-4583-ae5b-396f45daf21c",
   "metadata": {},
   "source": [
    "Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed7d401-37d1-4d15-b6dd-88956fff89e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed9c68-b7c5-4420-945d-84fa33fcf88f",
   "metadata": {},
   "source": [
    "## Stream debug events (stream_mode=\"debug\") {#debug}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94690715-f86c-42f6-be2d-4df82f6f9a96",
   "metadata": {},
   "source": [
    "Use this to stream **debug events** with as much information as possible for each step. Includes information about tasks that were scheduled to be executed as well as the results of the task executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6354f6-0c39-49cf-a529-b9c6c8713d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'task', 'timestamp': '2025-01-28T22:06:34.789803+00:00', 'step': 1, 'payload': {'id': 'eb305d74-3460-9510-d516-beed71a63414', 'name': 'refine_topic', 'input': {'topic': 'ice cream'}, 'triggers': ['start:refine_topic']}}\n",
      "{'type': 'task_result', 'timestamp': '2025-01-28T22:06:34.790013+00:00', 'step': 1, 'payload': {'id': 'eb305d74-3460-9510-d516-beed71a63414', 'name': 'refine_topic', 'error': None, 'result': [('topic', 'ice cream and cats')], 'interrupts': []}}\n",
      "{'type': 'task', 'timestamp': '2025-01-28T22:06:34.790165+00:00', 'step': 2, 'payload': {'id': '74355cb8-6284-25e0-579f-430493c1bdab', 'name': 'generate_joke', 'input': {'topic': 'ice cream and cats'}, 'triggers': ['refine_topic']}}\n",
      "{'type': 'task_result', 'timestamp': '2025-01-28T22:06:34.790337+00:00', 'step': 2, 'payload': {'id': '74355cb8-6284-25e0-579f-430493c1bdab', 'name': 'generate_joke', 'error': None, 'result': [('joke', 'This is a joke about ice cream and cats')], 'interrupts': []}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"debug\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791da60-0513-43e6-b445-788dd81683bb",
   "metadata": {},
   "source": [
    "## Stream LLM tokens ([stream_mode=\"messages\"](../streaming-tokens)) {#messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45d68b-f7ca-4012-96cc-d276a143f571",
   "metadata": {},
   "source": [
    "Use this to stream **LLM messages token-by-token** together with metadata for any LLM invocations inside nodes or tasks. Let's modify the above example to include LLM calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa787e1-be4d-433b-a1af-46a9c99ad8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    # highlight-next-line\n",
    "    llm_response = llm.invoke(\n",
    "        # highlight-next-line\n",
    "        [\n",
    "            # highlight-next-line\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a joke about {state['topic']}\"}\n",
    "            # highlight-next-line\n",
    "        ]\n",
    "        # highlight-next-line\n",
    "    )\n",
    "    return {\"joke\": llm_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c251f809-8922-46ea-bd5b-18264fcc523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| cat| sit| on| the| ice| cream| cone|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| \"|p|urr|-f|ect|\"| scoop|!| üç¶|üê±|"
     ]
    }
   ],
   "source": [
    "for message_chunk, metadata in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1912d72-7b68-4810-8b98-d7f3c35fbb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 2,\n",
       " 'langgraph_node': 'generate_joke',\n",
       " 'langgraph_triggers': ['refine_topic'],\n",
       " 'langgraph_path': ('__pregel_pull', 'generate_joke'),\n",
       " 'langgraph_checkpoint_ns': 'generate_joke:568879bc-8800-2b0d-a5b5-059526a4bebf',\n",
       " 'checkpoint_ns': 'generate_joke:568879bc-8800-2b0d-a5b5-059526a4bebf',\n",
       " 'ls_provider': 'openai',\n",
       " 'ls_model_name': 'gpt-4o-mini',\n",
       " 'ls_model_type': 'chat',\n",
       " 'ls_temperature': 0.7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ebeda-4498-40e0-a30a-0844cb491425",
   "metadata": {},
   "source": [
    "## Stream custom data (stream_mode=\"custom\") {#custom}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca56cc-d36e-4061-b1f6-9ade4e3e00a0",
   "metadata": {},
   "source": [
    "Use this to stream custom data from inside nodes using [`StreamWriter`][langgraph.types.StreamWriter]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bf6a2b-afe3-4bd3-8474-57cccd994f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import StreamWriter\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def generate_joke(state: State, writer: StreamWriter):\n",
    "    # highlight-next-line\n",
    "    writer({\"custom_key\": \"Writing custom data while generating a joke\"})\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ecfb0b0-3311-46f5-9dc8-6c7853373792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_key': 'Writing custom data while generating a joke'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e67f4d-fcab-46a8-93e2-b7bee30336c1",
   "metadata": {},
   "source": [
    "## Configure multiple streaming modes {#multiple}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff946a-f38d-42ad-bc71-a2621fab1b6c",
   "metadata": {},
   "source": [
    "Use this to combine multiple streaming modes. The outputs are streamed as tuples `(stream_mode, streamed_output)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4cab4b-356c-4276-9035-26974abe1efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream mode: updates\n",
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "\n",
      "\n",
      "Stream mode: custom\n",
      "{'custom_key': 'Writing custom data while generating a joke'}\n",
      "\n",
      "\n",
      "Stream mode: updates\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for stream_mode, chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "):\n",
    "    print(f\"Stream mode: {stream_mode}\")\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
