{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c4b04f-0c03-4321-9d40-38d12c59d088",
   "metadata": {},
   "source": [
    "# How to stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15403cdb-441d-43af-a29f-fc15abe03dcc",
   "metadata": {},
   "source": [
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n",
    "\n",
    "LangGraph is built with first class support for streaming. There are several different ways to stream back outputs from a graph run:\n",
    "\n",
    "- `\"values\"`: Emit all values in the state after each step.\n",
    "- `\"updates\"`: Emit only the node names and updates returned by the nodes after each step.\n",
    "    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.\n",
    "- `\"custom\"`: Emit custom data from inside nodes using `StreamWriter`.\n",
    "- [`\"messages\"`](../streaming-tokens): Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes.\n",
    "- `\"debug\"`: Emit debug events with as much information as possible for each step.\n",
    "\n",
    "You can stream outputs from the graph by using `graph.stream(..., stream_mode=<stream_mode>)` method, e.g.:\n",
    "\n",
    "=== \"Sync\"\n",
    "\n",
    "    ```python\n",
    "    for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "=== \"Async\"\n",
    "\n",
    "    ```python\n",
    "    async for chunk in graph.astream(inputs, stream_mode=\"updates\"):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "You can also combine multiple streaming mode by providing a list to `stream_mode` parameter:\n",
    "\n",
    "=== \"Sync\"\n",
    "\n",
    "    ```python\n",
    "    for chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "=== \"Async\"\n",
    "\n",
    "    ```python\n",
    "    async for chunk in graph.astream(inputs, stream_mode=[\"updates\", \"custom\"]):\n",
    "        print(chunk)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723cf76-6fe4-4b52-829f-3f28712ddcb7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427f8f66-7404-4c7d-a642-af5053b8b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03310ce6-e21f-4378-93bf-dd273fdb3e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80399508-bad8-43b7-8ec9-4c06ad1774cc",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4adbb2-61e8-4bb7-942d-b4dc27ba71ac",
   "metadata": {},
   "source": [
    "Let's define a simple graph with two nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4c513-1006-4179-bba9-d858fc952169",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faeb5ce8-d383-4277-b0a8-322e713638e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "\n",
    "\n",
    "def refine_topic(state: State):\n",
    "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
    "\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b90850-85bf-4391-b6b7-22ad45edaa3b",
   "metadata": {},
   "source": [
    "## Stream all values in the state (stream_mode=\"values\") {#values}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed60d4-cf78-4d4d-a660-6879539e168f",
   "metadata": {},
   "source": [
    "Use this to stream **all values** in the state after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daca06a-369b-41e5-8e4e-6edc4d4af3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'ice cream'}\n",
      "{'topic': 'ice cream and cats'}\n",
      "{'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb1bdb-f9fa-4d42-87ce-8e25d4290883",
   "metadata": {},
   "source": [
    "## Stream state updates from the nodes (stream_mode=\"updates\") {#updates}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c55326-d077-4583-ae5b-396f45daf21c",
   "metadata": {},
   "source": [
    "Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed7d401-37d1-4d15-b6dd-88956fff89e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed9c68-b7c5-4420-945d-84fa33fcf88f",
   "metadata": {},
   "source": [
    "## Stream debug events (stream_mode=\"debug\") {#debug}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94690715-f86c-42f6-be2d-4df82f6f9a96",
   "metadata": {},
   "source": [
    "Use this to stream **debug events** with as much information as possible for each step. Includes information about tasks that were scheduled to be executed as well as the results of the task executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6354f6-0c39-49cf-a529-b9c6c8713d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'task', 'timestamp': '2025-01-28T22:06:34.789803+00:00', 'step': 1, 'payload': {'id': 'eb305d74-3460-9510-d516-beed71a63414', 'name': 'refine_topic', 'input': {'topic': 'ice cream'}, 'triggers': ['start:refine_topic']}}\n",
      "{'type': 'task_result', 'timestamp': '2025-01-28T22:06:34.790013+00:00', 'step': 1, 'payload': {'id': 'eb305d74-3460-9510-d516-beed71a63414', 'name': 'refine_topic', 'error': None, 'result': [('topic', 'ice cream and cats')], 'interrupts': []}}\n",
      "{'type': 'task', 'timestamp': '2025-01-28T22:06:34.790165+00:00', 'step': 2, 'payload': {'id': '74355cb8-6284-25e0-579f-430493c1bdab', 'name': 'generate_joke', 'input': {'topic': 'ice cream and cats'}, 'triggers': ['refine_topic']}}\n",
      "{'type': 'task_result', 'timestamp': '2025-01-28T22:06:34.790337+00:00', 'step': 2, 'payload': {'id': '74355cb8-6284-25e0-579f-430493c1bdab', 'name': 'generate_joke', 'error': None, 'result': [('joke', 'This is a joke about ice cream and cats')], 'interrupts': []}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"debug\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791da60-0513-43e6-b445-788dd81683bb",
   "metadata": {},
   "source": [
    "## Stream LLM tokens ([stream_mode=\"messages\"](../streaming-tokens)) {#messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45d68b-f7ca-4012-96cc-d276a143f571",
   "metadata": {},
   "source": [
    "Use this to stream **LLM messages token-by-token** together with metadata for any LLM invocations inside nodes or tasks. Let's modify the above example to include LLM calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa787e1-be4d-433b-a1af-46a9c99ad8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    # highlight-next-line\n",
    "    llm_response = llm.invoke(\n",
    "        # highlight-next-line\n",
    "        [\n",
    "            # highlight-next-line\n",
    "            {\"role\": \"user\", \"content\": f\"Generate a joke about {state['topic']}\"}\n",
    "            # highlight-next-line\n",
    "        ]\n",
    "        # highlight-next-line\n",
    "    )\n",
    "    return {\"joke\": llm_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c251f809-8922-46ea-bd5b-18264fcc523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| cat| sit| on| the| ice| cream| cone|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| \"|p|urr|-f|ect|\"| scoop|!| üç¶|üê±|"
     ]
    }
   ],
   "source": [
    "for message_chunk, metadata in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1912d72-7b68-4810-8b98-d7f3c35fbb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 2,\n",
       " 'langgraph_node': 'generate_joke',\n",
       " 'langgraph_triggers': ['refine_topic'],\n",
       " 'langgraph_path': ('__pregel_pull', 'generate_joke'),\n",
       " 'langgraph_checkpoint_ns': 'generate_joke:568879bc-8800-2b0d-a5b5-059526a4bebf',\n",
       " 'checkpoint_ns': 'generate_joke:568879bc-8800-2b0d-a5b5-059526a4bebf',\n",
       " 'ls_provider': 'openai',\n",
       " 'ls_model_name': 'gpt-4o-mini',\n",
       " 'ls_model_type': 'chat',\n",
       " 'ls_temperature': 0.7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ebeda-4498-40e0-a30a-0844cb491425",
   "metadata": {},
   "source": [
    "## Stream custom data (stream_mode=\"custom\") {#custom}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca56cc-d36e-4061-b1f6-9ade4e3e00a0",
   "metadata": {},
   "source": [
    "Use this to stream custom data from inside nodes using [`StreamWriter`][langgraph.types.StreamWriter]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bf6a2b-afe3-4bd3-8474-57cccd994f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import StreamWriter\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def generate_joke(state: State, writer: StreamWriter):\n",
    "    # highlight-next-line\n",
    "    writer({\"custom_key\": \"Writing custom data while generating a joke\"})\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ecfb0b0-3311-46f5-9dc8-6c7853373792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_key': 'Writing custom data while generating a joke'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e67f4d-fcab-46a8-93e2-b7bee30336c1",
   "metadata": {},
   "source": [
    "## Configure multiple streaming modes {#multiple}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff946a-f38d-42ad-bc71-a2621fab1b6c",
   "metadata": {},
   "source": [
    "Use this to combine multiple streaming modes. The outputs are streamed as tuples `(stream_mode, streamed_output)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4cab4b-356c-4276-9035-26974abe1efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream mode: updates\n",
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "\n",
      "\n",
      "Stream mode: custom\n",
      "{'custom_key': 'Writing custom data while generating a joke'}\n",
      "\n",
      "\n",
      "Stream mode: updates\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for stream_mode, chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "):\n",
    "    print(f\"Stream mode: {stream_mode}\")\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to stream LLM tokens from your graph\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "When building LLM applications with LangGraph, you might want to stream individual LLM tokens from the LLM calls inside LangGraph nodes. You can do so via `graph.stream(..., stream_mode=\"messages\")`:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "def call_model(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(call_model)\n",
    "    ...\n",
    "    .compile()\n",
    "    \n",
    "for msg, metadata in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    print(msg)\n",
    "```\n",
    "\n",
    "The streamed outputs will be tuples of `(message chunk, metadata)`:\n",
    "\n",
    "* message chunk is the token streamed by the LLM\n",
    "* metadata is a dictionary with information about the graph node where the LLM was called as well as the LLM invocation metadata\n",
    "\n",
    "!!! note \"Using without LangChain\"\n",
    "\n",
    "    If you need to stream LLM tokens **without using LangChain**, you can use [`stream_mode=\"custom\"`](../streaming/#custom) to stream the outputs from LLM provider clients directly. Check out the [example below](#example-without-langchain) to learn more.\n",
    "\n",
    "!!! warning \"Async in Python < 3.11\"\n",
    "    \n",
    "    When using Python < 3.11 with async code, please ensure you manually pass the `RunnableConfig` through to the chat model when invoking it like so: `model.ainvoke(..., config)`.\n",
    "    The stream method collects all events from your nested code using a streaming tracer passed as a callback. In 3.11 and above, this is automatically handled via [contextvars](https://docs.python.org/3/library/contextvars.html); prior to 3.11, [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) lacked proper `contextvar` support, meaning that the callbacks will only propagate if you manually pass the config through. We do this in the `call_model` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b5425",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088bbd",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4",
   "metadata": {},
   "source": [
    "!!! note Manual Callback Propagation\n",
    "\n",
    "    Note that in `call_model(state: State, config: RunnableConfig):` below, we a) accept the [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) in the node function and b) pass it in as the second arg for `model.ainvoke(..., config)`. This is optional for python >= 3.11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c85b6-28f8-4c7f-843a-c05cb7fd7187",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbdd41-dff8-4118-8901-a619f91f3feb",
   "metadata": {},
   "source": [
    "Below we demonstrate an example with two LLM calls in a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc5905f-df82-4b31-84ad-2054f463aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Note: we're adding the tags here to be able to filter the model outputs down the line\n",
    "joke_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"joke\"])\n",
    "poem_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"poem\"])\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    poem: str\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "async def call_model(state, config):\n",
    "    topic = state[\"topic\"]\n",
    "    print(\"Writing joke...\")\n",
    "    # Note: Passing the config through explicitly is required for python < 3.11\n",
    "    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n",
    "    joke_response = await joke_model.ainvoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "    )\n",
    "    print(\"\\n\\nWriting poem...\")\n",
    "    poem_response = await poem_model.ainvoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "    )\n",
    "    return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96050fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n",
      "\n",
      "Writing poem...\n",
      "In| sun|lit| patches|,| sleek| and| sly|,|  \n",
      "|Wh|isk|ers| twitch| as| shadows| fly|.|  \n",
      "|With| velvet| paws| and| eyes| so| bright|,|  \n",
      "|They| dance| through| dreams|,| both| day| and| night|.|  \n",
      "\n",
      "|A| playful| p|ounce|,| a| gentle| p|urr|,|  \n",
      "|In| every| leap|,| a| soft| allure|.|  \n",
      "|Cur|led| in| warmth|,| a| silent| grace|,|  \n",
      "|Each| furry| friend|,| a| warm| embrace|.|  \n",
      "\n",
      "|Myst|ery| wrapped| in| fur| and| charm|,|  \n",
      "|A| soothing| presence|,| a| gentle| balm|.|  \n",
      "|In| their| gaze|,| the| world| slows| down|,|  \n",
      "|For| in| their| realm|,| we're| all| ren|own|.|"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if msg.content:\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdf561d-a5cd-4197-9c65-9ab8af85941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 1,\n",
       " 'langgraph_node': 'call_model',\n",
       " 'langgraph_triggers': ['start:call_model'],\n",
       " 'langgraph_path': ('__pregel_pull', 'call_model'),\n",
       " 'langgraph_checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n",
       " 'checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n",
       " 'ls_provider': 'openai',\n",
       " 'ls_model_name': 'gpt-4o-mini',\n",
       " 'ls_model_type': 'chat',\n",
       " 'ls_temperature': 0.7,\n",
       " 'tags': ['poem']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db91f8d-3e17-47f4-b45e-c72bbbcbb5ed",
   "metadata": {},
   "source": [
    "### Filter to specific LLM invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a72acd-98cc-43f6-9dbb-0e97d03d211b",
   "metadata": {},
   "source": [
    "You can see that we're streaming tokens from all of the LLM invocations. Let's now filter the streamed tokens to include only a specific LLM invocation. We can use the streamed metadata and filter events using the tags we've added to the LLMs previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e0df34-6020-445e-8ecd-ca4239e9b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n",
      "\n",
      "Writing poem...\n"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if msg.content and \"joke\" in metadata.get(\"tags\", []):\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fd3d7-a227-41ad-bd08-7ef994ab291b",
   "metadata": {},
   "source": [
    "## Example without LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699b3bab-9da7-4f2a-8006-93289350d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_client = AsyncOpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "async def stream_tokens(model_name: str, messages: list[dict]):\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        messages=messages, model=model_name, stream=True\n",
    "    )\n",
    "\n",
    "    role = None\n",
    "    async for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role is not None:\n",
    "            role = delta.role\n",
    "\n",
    "        if delta.content:\n",
    "            yield {\"role\": role, \"content\": delta.content}\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "async def call_model(state, config, writer):\n",
    "    topic = state[\"topic\"]\n",
    "    joke = \"\"\n",
    "    poem = \"\"\n",
    "\n",
    "    print(\"Writing joke...\")\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name, [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
    "    ):\n",
    "        joke += msg_chunk[\"content\"]\n",
    "        metadata = {**config[\"metadata\"], \"tags\": [\"joke\"]}\n",
    "        chunk_to_stream = (msg_chunk, metadata)\n",
    "        # highlight-next-line\n",
    "        writer(chunk_to_stream)\n",
    "\n",
    "    print(\"\\n\\nWriting poem...\")\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name, [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
    "    ):\n",
    "        poem += msg_chunk[\"content\"]\n",
    "        metadata = {**config[\"metadata\"], \"tags\": [\"poem\"]}\n",
    "        chunk_to_stream = (msg_chunk, metadata)\n",
    "        # highlight-next-line\n",
    "        writer(chunk_to_stream)\n",
    "\n",
    "    return {\"joke\": joke, \"poem\": poem}\n",
    "\n",
    "\n",
    "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af13d73-a0ea-44c0-a92e-28676cd164dd",
   "metadata": {},
   "source": [
    "!!! note \"stream_mode=\"custom\"\"\n",
    "\n",
    "    When streaming LLM tokens without LangChain, we recommend using [`stream_mode=\"custom\"`](../streaming/#stream-modecustom). This allows you to explicitly control which data from the LLM provider APIs to include in LangGraph streamed outputs, including any additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e977406d-7be6-4c9f-9185-5e5551f848f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the|\n",
      "\n",
      "Writing poem...\n",
      " mouse|!|In| sun|lit| patches|,| they| stretch| and| y|awn|,|  \n",
      "|With| whispered| paws| at| the| break| of| dawn|.|  \n",
      "|Wh|isk|ers| twitch| in| the| morning| light|,|  \n",
      "|Sil|ken| shadows|,| a| graceful| sight|.|  \n",
      "\n",
      "|The| gentle| p|urr|s|,| a| soothing| song|,|  \n",
      "|In| a| world| of| comfort|,| where| they| belong|.|  \n",
      "|M|yster|ious| hearts| wrapped| in| soft|est| fur|,|  \n",
      "|F|eline| whispers| in| every| p|urr|.|  \n",
      "\n",
      "|Ch|asing| dreams| on| a| moon|lit| chase|,|  \n",
      "|With| a| flick| of| a| tail|,| they| glide| with| grace|.|  \n",
      "|Oh|,| playful| spirits| of| whisk|ered| cheer|,|  \n",
      "|In| your| quiet| company|,| the| world| feels| near|.|  |"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(msg[\"content\"], end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdc1635-f424-4a5f-95db-e993bb16adb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 1,\n",
       " 'langgraph_node': 'call_model',\n",
       " 'langgraph_triggers': ['start:call_model'],\n",
       " 'langgraph_path': ('__pregel_pull', 'call_model'),\n",
       " 'langgraph_checkpoint_ns': 'call_model:3fa3fbe1-39d8-5209-dd77-0da38d4cc1c9',\n",
       " 'tags': ['poem']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afbbee-fab8-4c7f-ad26-094f8c8f4dd9",
   "metadata": {},
   "source": [
    "To filter to the specific LLM invocation, you can use the streamed metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdeee9d9-2625-403a-9253-418a0feeed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "\n",
      "\n",
      "Writing poem...\n",
      "In| shadows| soft|,| they| weave| and| play|,|  \n",
      "|With| whispered| paws|,| they| greet| the| day|.|  \n",
      "|Eyes| like| lantern|s|,| bright| and| keen|,|  \n",
      "|Guard|ians| of| secrets|,| unseen|,| serene|.|  \n",
      "\n",
      "|They| twist| and| stretch| in| sun|lit| beams|,|  \n",
      "|Ch|asing| the| echoes| of| half|-|formed| dreams|.|  \n",
      "|With| p|urring| songs| that| soothe| the| night|,|  \n",
      "|F|eline| spirits|,| pure| delight|.|  \n",
      "\n",
      "|On| windows|ills|,| they| perch| and| stare|,|  \n",
      "|Ad|vent|urers| bold| with| a| graceful| flair|.|  \n",
      "|In| every| leap| and| playful| bound|,|  \n",
      "|The| magic| of| cats|‚Äî|where| love| is| found|.|"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if \"poem\" in metadata.get(\"tags\", []):\n",
    "        print(msg[\"content\"], end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5889ca0-6feb-4864-a630-e97ccc2c587e",
   "metadata": {},
   "source": [
    "# How to stream LLM tokens from specific nodes\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "A common use case when [streaming LLM tokens](../streaming-tokens) is to only stream them from specific nodes. To do so, you can use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "def node_a(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "def node_b(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(node_a)\n",
    "    .add_node(node_b)\n",
    "    ...\n",
    "    .compile()\n",
    "    \n",
    "for msg, metadata in graph.stream(\n",
    "    inputs,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # stream from 'node_a'\n",
    "    # highlight-next-line\n",
    "    if metadata[\"langgraph_node\"] == \"node_a\":\n",
    "        print(msg)\n",
    "```\n",
    "\n",
    "!!! note \"Streaming from a specific LLM invocation\"\n",
    "\n",
    "    If you need to instead filter streamed LLM tokens to a specific LLM invocation, check out [this guide](../streaming-tokens#filter-to-specific-llm-invocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff85bd-8a5d-409e-93d4-e9242b5e976d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05157237-783c-49de-9f29-7dca3c285647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd22cd2-3152-433b-ad50-65be8ace61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce8c26-f38d-4bdb-89ff-b058e7560019",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419a3c71-7bf6-4656-99b8-b5d61f3f4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    poem: str\n",
    "\n",
    "\n",
    "def write_joke(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    joke_response = model.invoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
    "    )\n",
    "    return {\"joke\": joke_response.content}\n",
    "\n",
    "\n",
    "def write_poem(state: State):\n",
    "    topic = state[\"topic\"]\n",
    "    poem_response = model.invoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
    "    )\n",
    "    return {\"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(write_joke)\n",
    "    .add_node(write_poem)\n",
    "    # write both the joke and the poem concurrently\n",
    "    .add_edge(START, \"write_joke\")\n",
    "    .add_edge(START, \"write_poem\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed84d5e-ba10-4324-a664-dca263951a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In| shadows| soft|,| they| quietly| creep|,|  \n",
      "|Wh|isk|ered| wonders|,| in| dreams| they| leap|.|  \n",
      "|With| eyes| like| lantern|s|,| bright| and| wide|,|  \n",
      "|Myst|eries| linger| where| they| reside|.|  \n",
      "\n",
      "|P|aws| that| pat|ter| on| silent| floors|,|  \n",
      "|Cur|led| in| sun|be|ams|,| they| seek| out| more|.|  \n",
      "|A| flick| of| a| tail|,| a| leap|,| a| p|ounce|,|  \n",
      "|In| their| playful| world|,| we| can't| help| but| bounce|.|  \n",
      "\n",
      "|Guard|ians| of| secrets|,| with| gentle| grace|,|  \n",
      "|Each| little| me|ow|,| a| warm| embrace|.|  \n",
      "|Oh|,| the| joy| that| they| bring|,| so| pure| and| true|,|  \n",
      "|In| the| heart| of| a| cat|,| there's| magic| anew|.|  |"
     ]
    }
   ],
   "source": [
    "for msg, metadata in graph.stream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef56c4-6218-4cb9-b66c-7aa37a520754",
   "metadata": {},
   "source": [
    "# How to stream from subgraphs\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Subgraphs](../../concepts/low_level/#subgraphs)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "If you have created a graph with [subgraphs](../subgraph), you may wish to stream outputs from those subgraphs. To do so, you can specify `subgraphs=True` in parent graph's `.stream()` method:\n",
    "\n",
    "\n",
    "```python\n",
    "for chunk in parent_graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    # highlight-next-line\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "First let's install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc9d90e-74cf-4cdb-a9ad-046e585bd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb838bb-4fea-46f5-bb10-8552a41a2b70",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57e693-fcf9-4dff-85c3-877e3d660a45",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fdaa67-c521-4917-b4ea-1eb0e83c289e",
   "metadata": {},
   "source": [
    "Let's define a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42e4ce5-ceb5-4a95-bf69-399342a9c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d41a1-f88d-49b1-b141-ffa1212d0ca3",
   "metadata": {},
   "source": [
    "Let's now stream the outputs from the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06be7be7-200b-4e6e-a92b-5cea867fe617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_1': {'foo': 'hi! foo'}}\n",
      "{'node_2': {'foo': 'hi! foobar'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"foo\": \"foo\"}, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764ef35-1eea-40de-b04f-b20f010513ce",
   "metadata": {},
   "source": [
    "You can see that we're only emitting the updates from the parent graph nodes (`node_1` and `node_2`). To emit the updates from the _subgraph_ nodes you can specify `subgraphs=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee184746-5a98-4f60-a548-a82e04e858ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:b692b345-cfb3-b709-628c-f0ba9608f72e',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:b692b345-cfb3-b709-628c-f0ba9608f72e',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"updates\",\n",
    "    # highlight-next-line\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0222a4a-73ab-4bee-b561-e09117c88ab5",
   "metadata": {},
   "source": [
    "Voila! The streamed outputs now contain updates from both the parent graph and the subgraph. **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "695d935e-b4fe-45a6-a061-a66d32cb832b",
   "metadata": {},
   "source": [
    "# How to stream data from within a tool\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "    - [Tools](https://python.langchain.com/docs/concepts/tools/)\n",
    "\n",
    "If your graph calls tools that use LLMs or any other streaming APIs, you might want to surface partial results during the execution of the tool, especially if the tool takes a longer time to run.\n",
    "\n",
    "1. To stream **arbitrary** data from inside a tool you can use [`stream_mode=\"custom\"`](../streaming#custom) and `get_stream_writer()`:\n",
    "\n",
    "    ```python\n",
    "    # highlight-next-line\n",
    "    from langgraph.config import get_stream_writer\n",
    "    \n",
    "    def tool(tool_arg: str):\n",
    "        writer = get_stream_writer()\n",
    "        for chunk in custom_data_stream():\n",
    "            # stream any arbitrary data\n",
    "            # highlight-next-line\n",
    "            writer(chunk)\n",
    "        ...\n",
    "    \n",
    "    for chunk in graph.stream(\n",
    "        inputs,\n",
    "        # highlight-next-line\n",
    "        stream_mode=\"custom\"\n",
    "    ):\n",
    "        print(chunk)\n",
    "    ```\n",
    "\n",
    "2. To stream LLM tokens generated by a tool calling an LLM you can use [`stream_mode=\"messages\"`](../streaming#messages):\n",
    "\n",
    "    ```python\n",
    "    from langgraph.graph import StateGraph, MessagesState\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    model = ChatOpenAI()\n",
    "    \n",
    "    def tool(tool_arg: str):\n",
    "        model.invoke(tool_arg)\n",
    "        ...\n",
    "    \n",
    "    def call_tools(state: MessagesState):\n",
    "        tool_call = get_tool_call(state)\n",
    "        tool_result = tool(**tool_call[\"args\"])\n",
    "        ...\n",
    "    \n",
    "    graph = (\n",
    "        StateGraph(MessagesState)\n",
    "        .add_node(call_tools)\n",
    "        ...\n",
    "        .compile()\n",
    "    \n",
    "    for msg, metadata in graph.stream(\n",
    "        inputs,\n",
    "        # highlight-next-line\n",
    "        stream_mode=\"messages\"\n",
    "    ):\n",
    "        print(msg)\n",
    "    ```\n",
    "\n",
    "!!! note \"Using without LangChain\"\n",
    "\n",
    "    If you need to stream data from inside tools **without using LangChain**, you can use [`stream_mode=\"custom\"`](../streaming/#custom). Check out the [example below](#example-without-langchain) to learn more.\n",
    "\n",
    "!!! warning \"Async in Python < 3.11\"\n",
    "    \n",
    "    When using Python < 3.11 with async code, please ensure you manually pass the `RunnableConfig` through to the chat model when invoking it like so: `model.ainvoke(..., config)`.\n",
    "    The stream method collects all events from your nested code using a streaming tracer passed as a callback. In 3.11 and above, this is automatically handled via [contextvars](https://docs.python.org/3/library/contextvars.html); prior to 3.11, [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) lacked proper `contextvar` support, meaning that the callbacks will only propagate if you manually pass the config through. We do this in the `call_model` function below.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b364dfe2-010b-4588-8489-fb4d8be1f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf6b41d-7fcb-40b6-9a72-229cdd00a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767cd76a",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ddc3ff-5620-48de-82f0-03b9137410cf",
   "metadata": {},
   "source": [
    "## Streaming custom data\n",
    "\n",
    "We'll use a [prebuilt ReAct agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] for this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1975577-a485-42bd-b0f1-d3e987faf52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "\n",
    "@tool\n",
    "async def get_items(place: str) -> str:\n",
    "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n",
    "    # highlight-next-line\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # this can be replaced with any actual streaming logic that you might have\n",
    "    items = [\"books\", \"penciles\", \"pictures\"]\n",
    "    for chunk in items:\n",
    "        # highlight-next-line\n",
    "        writer({\"custom_tool_data\": chunk})\n",
    "\n",
    "    return \", \".join(items)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "tools = [get_items]\n",
    "# contains `agent` (tool-calling LLM) and `tools` (tool executor) nodes\n",
    "agent = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96d572-d15f-4f00-b629-cf25e0b4dece",
   "metadata": {},
   "source": [
    "Let's now invoke our agent with an input that requires a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae5051c-53b9-4c53-87b2-d7263cda3b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_tool_data': 'books'}\n",
      "{'custom_tool_data': 'penciles'}\n",
      "{'custom_tool_data': 'pictures'}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [  # noqa\n",
    "        {\"role\": \"user\", \"content\": \"what items are in the office?\"}\n",
    "    ]\n",
    "}\n",
    "async for chunk in agent.astream(\n",
    "    inputs,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fa9fc-19af-47d6-9031-ee1720c51aa2",
   "metadata": {},
   "source": [
    "## Streaming LLM tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eaf453-9773-424d-a110-9e1038a69805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessageChunk\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "@tool\n",
    "async def get_items(\n",
    "    place: str,\n",
    "    # Manually accept config (needed for Python <= 3.10)\n",
    "    # highlight-next-line\n",
    "    config: RunnableConfig,\n",
    ") -> str:\n",
    "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n",
    "    # Attention: when using async, you should be invoking the LLM using ainvoke!\n",
    "    # If you fail to do so, streaming will NOT work.\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n",
    "                    \"List at least 3 such items separating them by a comma. And include a brief description of each item.\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "\n",
    "tools = [get_items]\n",
    "# contains `agent` (tool-calling LLM) and `tools` (tool executor) nodes\n",
    "agent = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9cdad3-3e9a-444f-9d9d-eae20b8d3486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly|!| Here| are| three| items| you| might| find| in| a| bedroom|:\n",
      "\n",
      "|1|.| **|Bed|**|:| The| central| piece| of| furniture| in| a| bedroom|,| typically| consisting| of| a| mattress| supported| by| a| frame|.| It| is| designed| for| sleeping| and| can| vary| in| size| from| twin| to| king|.| Beds| often| have| bedding|,| including| sheets|,| pillows|,| and| comfort|ers|,| to| enhance| comfort|.\n",
      "\n",
      "|2|.| **|D|resser|**|:| A| piece| of| furniture| with| drawers| used| for| storing| clothing| and| personal| items|.| Dress|ers| often| have| a| flat| surface| on| top|,| which| can| be| used| for| decorative| items|,| a| mirror|,| or| personal| accessories|.| They| help| keep| the| bedroom| organized| and| clutter|-free|.\n",
      "\n",
      "|3|.| **|Night|stand|**|:| A| small| table| or| cabinet| placed| beside| the| bed|,| used| for| holding| items| such| as| a| lamp|,| alarm| clock|,| books|,| or| personal| items|.| Night|stands| provide| convenience| for| easy| access| to| essentials| during| the| night|,| adding| functionality| and| style| to| the| bedroom| decor|.|"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [  # noqa\n",
    "        {\"role\": \"user\", \"content\": \"what items are in the bedroom?\"}\n",
    "    ]\n",
    "}\n",
    "async for msg, metadata in agent.astream(\n",
    "    inputs,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if (\n",
    "        isinstance(msg, AIMessageChunk)\n",
    "        and msg.content\n",
    "        # Stream all messages from the tool node\n",
    "        # highlight-next-line\n",
    "        and metadata[\"langgraph_node\"] == \"tools\"\n",
    "    ):\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598d7e2-617d-4c06-bc9a-6a03d5f58499",
   "metadata": {},
   "source": [
    "## Example without LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ddcb6-63a7-4c83-a739-bafbe3cd135a",
   "metadata": {},
   "source": [
    "You can also stream data from within tool invocations **without using LangChain**. Below example demonstrates how to do it for a graph with a single tool-executing node. We'll leave it as an exercise for the reader to [implement ReAct agent from scratch](../react-agent-from-scratch) without using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8be67f-4bb8-4f14-9fdb-fc60340f3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "\n",
    "from typing import TypedDict\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_client = AsyncOpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "async def stream_tokens(model_name: str, messages: list[dict]):\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        messages=messages, model=model_name, stream=True\n",
    "    )\n",
    "    role = None\n",
    "    async for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role is not None:\n",
    "            role = delta.role\n",
    "\n",
    "        if delta.content:\n",
    "            yield {\"role\": role, \"content\": delta.content}\n",
    "\n",
    "\n",
    "# this is our tool\n",
    "async def get_items(place: str) -> str:\n",
    "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n",
    "    # highlight-next-line\n",
    "    writer = get_stream_writer()\n",
    "    response = \"\"\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name,\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Can you tell me what kind of items \"\n",
    "                    f\"i might find in the following place: '{place}'. \"\n",
    "                    \"List at least 3 such items separating them by a comma. \"\n",
    "                    \"And include a brief description of each item.\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "    ):\n",
    "        response += msg_chunk[\"content\"]\n",
    "        # highlight-next-line\n",
    "        writer(msg_chunk)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[dict], operator.add]\n",
    "\n",
    "\n",
    "# this is the tool-calling graph node\n",
    "async def call_tool(state: State):\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    tool_call = ai_message[\"tool_calls\"][-1]\n",
    "\n",
    "    function_name = tool_call[\"function\"][\"name\"]\n",
    "    if function_name != \"get_items\":\n",
    "        raise ValueError(f\"Tool {function_name} not supported\")\n",
    "\n",
    "    function_arguments = tool_call[\"function\"][\"arguments\"]\n",
    "    arguments = json.loads(function_arguments)\n",
    "\n",
    "    function_response = await get_items(**arguments)\n",
    "    tool_message = {\n",
    "        \"tool_call_id\": tool_call[\"id\"],\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "    }\n",
    "    return {\"messages\": [tool_message]}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)  # noqa\n",
    "    .add_node(call_tool)\n",
    "    .add_edge(START, \"call_tool\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e712d12-841c-4eac-a4d8-d01c73c86c8c",
   "metadata": {},
   "source": [
    "Let's now invoke our graph with an AI message that contains a tool call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c30c7b4-62df-4855-8219-d5e1a1a09be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure|!| Here| are| three| common| items| you| might| find| in| a| bedroom|:\n",
      "\n",
      "|1|.| **|Bed|**|:| The| focal| point| of| the| bedroom|,| a| bed| typically| consists| of| a| mattress| resting| on| a| frame|,| and| it| may| include| pillows| and| bedding|.| It| provides| a| comfortable| place| for| sleeping| and| resting|.\n",
      "\n",
      "|2|.| **|D|resser|**|:| A| piece| of| furniture| with| multiple| drawers|,| a| dresser| is| used| for| storing| clothes|,| accessories|,| and| personal| items|.| It| often| has| a| flat| surface| that| may| be| used| to| display| decorative| items| or| a| mirror|.\n",
      "\n",
      "|3|.| **|Night|stand|**|:| Also| known| as| a| bedside| table|,| a| night|stand| is| placed| next| to| the| bed| and| typically| holds| items| like| lamps|,| books|,| alarm| clocks|,| and| personal| belongings| for| convenience| during| the| night|.\n",
      "\n",
      "|These| items| contribute| to| the| functionality| and| comfort| of| the| bedroom| environment|.|"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": None,\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"1\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": '{\"place\":\"bedroom\"}',\n",
    "                        \"name\": \"get_items\",\n",
    "                    },\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "async for chunk in graph.astream(\n",
    "    inputs,\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk[\"content\"], end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fd6c8",
   "metadata": {},
   "source": [
    "# How to stream from subgraphs\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Subgraphs](../../concepts/low_level/#subgraphs)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "If you have created a graph with [subgraphs](../subgraph), you may wish to stream outputs from those subgraphs. To do so, you can specify `subgraphs=True` in parent graph's `.stream()` method:\n",
    "\n",
    "\n",
    "```python\n",
    "for chunk in parent_graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    # highlight-next-line\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(chunk)\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "First let's install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b839bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14859e23",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph ‚Äî read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0908ce",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ec1f7",
   "metadata": {},
   "source": [
    "Let's define a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6747cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "# Define subgraph\n",
    "class SubgraphState(TypedDict):\n",
    "    foo: str  # note that this key is shared with the parent graph state\n",
    "    bar: str\n",
    "\n",
    "\n",
    "def subgraph_node_1(state: SubgraphState):\n",
    "    return {\"bar\": \"bar\"}\n",
    "\n",
    "\n",
    "def subgraph_node_2(state: SubgraphState):\n",
    "    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n",
    "\n",
    "\n",
    "subgraph_builder = StateGraph(SubgraphState)\n",
    "subgraph_builder.add_node(subgraph_node_1)\n",
    "subgraph_builder.add_node(subgraph_node_2)\n",
    "subgraph_builder.add_edge(START, \"subgraph_node_1\")\n",
    "subgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\n",
    "subgraph = subgraph_builder.compile()\n",
    "\n",
    "\n",
    "# Define parent graph\n",
    "class ParentState(TypedDict):\n",
    "    foo: str\n",
    "\n",
    "\n",
    "def node_1(state: ParentState):\n",
    "    return {\"foo\": \"hi! \" + state[\"foo\"]}\n",
    "\n",
    "\n",
    "builder = StateGraph(ParentState)\n",
    "builder.add_node(\"node_1\", node_1)\n",
    "builder.add_node(\"node_2\", subgraph)\n",
    "builder.add_edge(START, \"node_1\")\n",
    "builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf0948",
   "metadata": {},
   "source": [
    "Let's now stream the outputs from the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511337de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_1': {'foo': 'hi! foo'}}\n",
      "{'node_2': {'foo': 'hi! foobar'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream({\"foo\": \"foo\"}, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c575540",
   "metadata": {},
   "source": [
    "You can see that we're only emitting the updates from the parent graph nodes (`node_1` and `node_2`). To emit the updates from the _subgraph_ nodes you can specify `subgraphs=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0233ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((), {'node_1': {'foo': 'hi! foo'}})\n",
      "(('node_2:b692b345-cfb3-b709-628c-f0ba9608f72e',), {'subgraph_node_1': {'bar': 'bar'}})\n",
      "(('node_2:b692b345-cfb3-b709-628c-f0ba9608f72e',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n",
      "((), {'node_2': {'foo': 'hi! foobar'}})\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"foo\": \"foo\"},\n",
    "    stream_mode=\"updates\",\n",
    "    # highlight-next-line\n",
    "    subgraphs=True,\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc9b9e",
   "metadata": {},
   "source": [
    "Voila! The streamed outputs now contain updates from both the parent graph and the subgraph. **Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
