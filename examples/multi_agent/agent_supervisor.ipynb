{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
   "metadata": {},
   "source": [
    "## Руководитель агентов\n",
    "\n",
    "В [предыдущем примере](multi-agent-collaboration.ipynb) вы можете ознакомиться с автоматической маршрутизацией сообщений в зависимости от результата работы первичного агента-исследователя.\n",
    "\n",
    "Кроме такого подхода вы можете использовать LLM для организации работы агентов.\n",
    "\n",
    "Пример ниже показывает как создать группу агентов с LLM в роли руководителя, который помогает распределять задачи.\n",
    "\n",
    "![Схема](./img/supervisor-diagram.png)\n",
    "\n",
    "Для упрощения кода каждой из вершин, в которых распологаются агенты, используется класс `AgentExecutor` библиотеки LangChain. Таким образом этот и другие сложные примеры показывают различные подходы к проектированию с помощью LangGraph. Комбинируйте эти подходы с другими базовыми решениями, для более эффективного решения своих задач. \n",
    "\n",
    "Перед началом разработки подготовьте среду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d30b6f7-3bec-4d9f-af50-43dfdc81ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install -U langchain langchain_openai langchain_experimental langsmith pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c2f3de-c730-4aec-85a6-af2c2f058803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Optional, add tracing in LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac25624-4d83-45a4-b9ef-a10589aacfb7",
   "metadata": {},
   "source": [
    "## Создание инструментов\n",
    "\n",
    "Для этого примера вам потребуется два агента. Первый будет выполнять исследование с помощью запросов к поисковой системе, а второй будет отрисовывать графики.\n",
    "\n",
    "Задайте инструменты, которые будут использовать каждый из агентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c6778-403b-4b49-9b93-678e910d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# Функция выполняет код локально, что может быть небезсопасно\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1e85-22d4-4c22-9062-72a346a0d709",
   "metadata": {},
   "source": [
    "## Создание вспомогательных функций\n",
    "\n",
    "Создайте вспомогательную функцию, которая упростит добавление вершин с агентами-исполнителями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4823dd9-26bd-4e1a-8117-b97b2860211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Каждой вершине с исполнителем присваивается имя и дается доступ\n",
    "    # к определенным инструментам\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c302b0-cd57-4913-986f-5dc7d6d77386",
   "metadata": {},
   "source": [
    "Также определите функцию, которая будет представлять вершины графа. Функция будет преобразовывать ответ агента в сообщения от человека (`HumanMessage`). Это нужно для работы с глобальным состоянием графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80862241-a1a7-4726-bce5-f867b233832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32962d2-5487-496d-aefc-2a3b0d194985",
   "metadata": {},
   "source": [
    "### Создайте руководителя агентов\n",
    "\n",
    "Руководитель вызывает функцию, чтобы определенить следующую вершину-исполнителя или завершить обработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311f0a58-b425-4496-adac-dc4cd8ffb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"Coder\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Руководитель агентов — это LLM-вершина графа. Руководитель определяет\n",
    "# какому из агентов передать задачу и принимает решение о ее выполнении\n",
    "options = [\"FINISH\"] + members\n",
    "# Использование openai-функций упрощает обработку вывода\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d507f-34d1-4f1b-8dde-5e58d17b2166",
   "metadata": {},
   "source": [
    "## Соберите граф\n",
    "\n",
    "Теперь вы можете собрать граф.\n",
    "Для этого определите состояние агента и вершины-исполнители. Для определения вершин используйте созданную вспомогательную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a430af7-8fce-4e66-ba9e-d940c1bc48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# Состояние агента передается на вход каждой из вершин графа\n",
    "class AgentState(TypedDict):\n",
    "    # Аннотация указывает графу, что новые сообщения нужно всегда\n",
    "    # добавлять к текущим состояниям\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # Поле 'next' указывает, что делать дальше\n",
    "    next: str\n",
    "\n",
    "\n",
    "research_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# NOTE: НА ЭТОМ ЭТАПЕ ВЫПОЛНЯЕТСЯ ПРОИЗВОЛЬНЫЙ КОД. БУДЬТЕ ОСТОРОЖНЫ\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Researcher\", research_node)\n",
    "workflow.add_node(\"Coder\", code_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1593d5-39f7-4819-96d2-4ad7d7991d72",
   "metadata": {},
   "source": [
    "Соедините все ребра графа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14778e86-077b-4e6a-893c-400e59b0cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for member in members:\n",
    "    # Исполнители должны ВСЕГДА докладывать руководителю о завершении работы\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# Руководитель заполняет поле \"next\" в состостоянии графа,\n",
    "# которое переходит к другой вершине или заканчивает обработку\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Добавьте точку входа\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36496de-7121-4c49-8cb6-58c943c66628",
   "metadata": {},
   "source": [
    "## Запустите граф\n",
    "\n",
    "Теперь, когда граф собран, вы можете запустить его и посмотреть, как он работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ba78e9-d9c1-457c-a073-d606d5d3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Coder'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coder': {'messages': [HumanMessage(content=\"The code `print('Hello, World!')` was executed, and the output is:\\n\\n```\\nHello, World!\\n```\", name='Coder')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Code hello world and print it to the terminal\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a92dfd-0e11-47f5-aad4-b68d24990e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Researcher'}}\n",
      "----\n",
      "{'Researcher': {'messages': [HumanMessage(content='**Research Report on Pikas**\\n\\nPikas are small mammals related to rabbits, known for their distinctive chirping sounds. They inhabit some of the most challenging environments, particularly boulder fields at high elevations, such as those found along the treeless slopes of the Southern Rockies, where they can be found at altitudes of up to 14,000 feet. Pikas are well-adapted to cold climates and typically do not fare well in warmer temperatures.\\n\\nRecent studies have shown that pikas are being impacted by climate change. Research by Peter Billman, a Ph.D. student from the University of Connecticut, indicates that pikas have moved upslope by approximately 1,160 feet. This upslope retreat is a direct response to changing climatic conditions, as pikas seek cooler temperatures at higher elevations.\\n\\nPikas are also known to be industrious foragers, particularly during the summer months when they gather vegetation to create haypiles for winter sustenance. Their behavior is encapsulated in the saying, \"making hay while the sun shines,\" reflecting their proactive approach to survival in harsh conditions.\\n\\nThe effects of climate change on pikas are not limited to the Southern Rockies. Studies published in Global Change Biology suggest that climate change is influencing pikas even in areas where they were previously thought to be less vulnerable, such as the Northern Rockies. These findings point to a broader trend of pikas moving to higher elevations, a behavior that may indicate a search for cooler, more suitable habitats.\\n\\nMoreover, researchers are exploring the possibility that pikas at lower elevations may have developed warm adaptations that could be beneficial for their future survival, given the ongoing climatic shifts. This line of research could help conservationists understand how pikas might cope with a warming world.\\n\\nIn conclusion, pikas are a species that not only fascinate with their unique behaviors and adaptations but also serve as indicators of environmental changes. Their upslope migration in response to climate change highlights the urgency for understanding and mitigating the effects of global warming on mountain ecosystems and the species that inhabit them.\\n\\n**Sources:**\\n- [Colorado Sun](https://coloradosun.com/2023/08/27/colorado-pika-population-climate-change/)\\n- [Wildlife.org](https://wildlife.org/climate-change-affects-pikas-even-in-unlikely-areas/)', name='Researcher')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Write a brief research report on pikas.\")]},\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d363d2c-e0da-4cce-ba47-ad2aa9df0fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
