{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to stream LLM tokens from your graph\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "\n",
    "    This guide assumes familiarity with the following:\n",
    "    \n",
    "    - [Streaming](../../concepts/streaming/)\n",
    "    - [Chat Models](https://python.langchain.com/docs/concepts/chat_models/)\n",
    "\n",
    "When building LLM applications with LangGraph, you might want to stream individual LLM tokens from the LLM calls inside LangGraph nodes. You can do so via `graph.stream(..., stream_mode=\"messages\")`:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "def call_model(state: State):\n",
    "    model.invoke(...)\n",
    "    ...\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(call_model)\n",
    "    ...\n",
    "    .compile()\n",
    "    \n",
    "for msg, metadata in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    print(msg)\n",
    "```\n",
    "\n",
    "The streamed outputs will be tuples of `(message chunk, metadata)`:\n",
    "\n",
    "* message chunk is the token streamed by the LLM\n",
    "* metadata is a dictionary with information about the graph node where the LLM was called as well as the LLM invocation metadata\n",
    "\n",
    "!!! note \"Using without LangChain\"\n",
    "\n",
    "    If you need to stream LLM tokens **without using LangChain**, you can use [`stream_mode=\"custom\"`](../streaming/#custom) to stream the outputs from LLM provider clients directly. Check out the [example below](#example-without-langchain) to learn more.\n",
    "\n",
    "!!! warning \"Async in Python < 3.11\"\n",
    "    \n",
    "    When using Python < 3.11 with async code, please ensure you manually pass the `RunnableConfig` through to the chat model when invoking it like so: `model.ainvoke(..., config)`.\n",
    "    The stream method collects all events from your nested code using a streaming tracer passed as a callback. In 3.11 and above, this is automatically handled via [contextvars](https://docs.python.org/3/library/contextvars.html); prior to 3.11, [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) lacked proper `contextvar` support, meaning that the callbacks will only propagate if you manually pass the config through. We do this in the `call_model` function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b5425",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088bbd",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5094-9297-4d19-a04e-3eedc75cefb4",
   "metadata": {},
   "source": [
    "!!! note Manual Callback Propagation\n",
    "\n",
    "    Note that in `call_model(state: State, config: RunnableConfig):` below, we a) accept the [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) in the node function and b) pass it in as the second arg for `model.ainvoke(..., config)`. This is optional for python >= 3.11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c85b6-28f8-4c7f-843a-c05cb7fd7187",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbdd41-dff8-4118-8901-a619f91f3feb",
   "metadata": {},
   "source": [
    "Below we demonstrate an example with two LLM calls in a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc5905f-df82-4b31-84ad-2054f463aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Note: we're adding the tags here to be able to filter the model outputs down the line\n",
    "joke_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"joke\"])\n",
    "poem_model = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"poem\"])\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    poem: str\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "async def call_model(state, config):\n",
    "    topic = state[\"topic\"]\n",
    "    print(\"Writing joke...\")\n",
    "    # Note: Passing the config through explicitly is required for python < 3.11\n",
    "    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n",
    "    joke_response = await joke_model.ainvoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "    )\n",
    "    print(\"\\n\\nWriting poem...\")\n",
    "    poem_response = await poem_model.ainvoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n",
    "        # highlight-next-line\n",
    "        config,\n",
    "    )\n",
    "    return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96050fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n",
      "\n",
      "Writing poem...\n",
      "In| sun|lit| patches|,| sleek| and| sly|,|  \n",
      "|Wh|isk|ers| twitch| as| shadows| fly|.|  \n",
      "|With| velvet| paws| and| eyes| so| bright|,|  \n",
      "|They| dance| through| dreams|,| both| day| and| night|.|  \n",
      "\n",
      "|A| playful| p|ounce|,| a| gentle| p|urr|,|  \n",
      "|In| every| leap|,| a| soft| allure|.|  \n",
      "|Cur|led| in| warmth|,| a| silent| grace|,|  \n",
      "|Each| furry| friend|,| a| warm| embrace|.|  \n",
      "\n",
      "|Myst|ery| wrapped| in| fur| and| charm|,|  \n",
      "|A| soothing| presence|,| a| gentle| balm|.|  \n",
      "|In| their| gaze|,| the| world| slows| down|,|  \n",
      "|For| in| their| realm|,| we're| all| ren|own|.|"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if msg.content:\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdf561d-a5cd-4197-9c65-9ab8af85941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 1,\n",
       " 'langgraph_node': 'call_model',\n",
       " 'langgraph_triggers': ['start:call_model'],\n",
       " 'langgraph_path': ('__pregel_pull', 'call_model'),\n",
       " 'langgraph_checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n",
       " 'checkpoint_ns': 'call_model:6ddc5f0f-1dd0-325d-3014-f949286ce595',\n",
       " 'ls_provider': 'openai',\n",
       " 'ls_model_name': 'gpt-4o-mini',\n",
       " 'ls_model_type': 'chat',\n",
       " 'ls_temperature': 0.7,\n",
       " 'tags': ['poem']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db91f8d-3e17-47f4-b45e-c72bbbcbb5ed",
   "metadata": {},
   "source": [
    "### Filter to specific LLM invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a72acd-98cc-43f6-9dbb-0e97d03d211b",
   "metadata": {},
   "source": [
    "You can see that we're streaming tokens from all of the LLM invocations. Let's now filter the streamed tokens to include only a specific LLM invocation. We can use the streamed metadata and filter events using the tags we've added to the LLMs previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e0df34-6020-445e-8ecd-ca4239e9b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the| mouse|!|\n",
      "\n",
      "Writing poem...\n"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if msg.content and \"joke\" in metadata.get(\"tags\", []):\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fd3d7-a227-41ad-bd08-7ef994ab291b",
   "metadata": {},
   "source": [
    "## Example without LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699b3bab-9da7-4f2a-8006-93289350d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_client = AsyncOpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "async def stream_tokens(model_name: str, messages: list[dict]):\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        messages=messages, model=model_name, stream=True\n",
    "    )\n",
    "\n",
    "    role = None\n",
    "    async for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role is not None:\n",
    "            role = delta.role\n",
    "\n",
    "        if delta.content:\n",
    "            yield {\"role\": role, \"content\": delta.content}\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "async def call_model(state, config, writer):\n",
    "    topic = state[\"topic\"]\n",
    "    joke = \"\"\n",
    "    poem = \"\"\n",
    "\n",
    "    print(\"Writing joke...\")\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name, [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n",
    "    ):\n",
    "        joke += msg_chunk[\"content\"]\n",
    "        metadata = {**config[\"metadata\"], \"tags\": [\"joke\"]}\n",
    "        chunk_to_stream = (msg_chunk, metadata)\n",
    "        # highlight-next-line\n",
    "        writer(chunk_to_stream)\n",
    "\n",
    "    print(\"\\n\\nWriting poem...\")\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name, [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n",
    "    ):\n",
    "        poem += msg_chunk[\"content\"]\n",
    "        metadata = {**config[\"metadata\"], \"tags\": [\"poem\"]}\n",
    "        chunk_to_stream = (msg_chunk, metadata)\n",
    "        # highlight-next-line\n",
    "        writer(chunk_to_stream)\n",
    "\n",
    "    return {\"joke\": joke, \"poem\": poem}\n",
    "\n",
    "\n",
    "graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af13d73-a0ea-44c0-a92e-28676cd164dd",
   "metadata": {},
   "source": [
    "!!! note \"stream_mode=\"custom\"\"\n",
    "\n",
    "    When streaming LLM tokens without LangChain, we recommend using [`stream_mode=\"custom\"`](../streaming/#stream-modecustom). This allows you to explicitly control which data from the LLM provider APIs to include in LangGraph streamed outputs, including any additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e977406d-7be6-4c9f-9185-5e5551f848f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was| the| cat| sitting| on| the| computer|?\n",
      "\n",
      "|Because| it| wanted| to| keep| an| eye| on| the|\n",
      "\n",
      "Writing poem...\n",
      " mouse|!|In| sun|lit| patches|,| they| stretch| and| y|awn|,|  \n",
      "|With| whispered| paws| at| the| break| of| dawn|.|  \n",
      "|Wh|isk|ers| twitch| in| the| morning| light|,|  \n",
      "|Sil|ken| shadows|,| a| graceful| sight|.|  \n",
      "\n",
      "|The| gentle| p|urr|s|,| a| soothing| song|,|  \n",
      "|In| a| world| of| comfort|,| where| they| belong|.|  \n",
      "|M|yster|ious| hearts| wrapped| in| soft|est| fur|,|  \n",
      "|F|eline| whispers| in| every| p|urr|.|  \n",
      "\n",
      "|Ch|asing| dreams| on| a| moon|lit| chase|,|  \n",
      "|With| a| flick| of| a| tail|,| they| glide| with| grace|.|  \n",
      "|Oh|,| playful| spirits| of| whisk|ered| cheer|,|  \n",
      "|In| your| quiet| company|,| the| world| feels| near|.|  |"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    # highlight-next-line\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(msg[\"content\"], end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdc1635-f424-4a5f-95db-e993bb16adb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'langgraph_step': 1,\n",
       " 'langgraph_node': 'call_model',\n",
       " 'langgraph_triggers': ['start:call_model'],\n",
       " 'langgraph_path': ('__pregel_pull', 'call_model'),\n",
       " 'langgraph_checkpoint_ns': 'call_model:3fa3fbe1-39d8-5209-dd77-0da38d4cc1c9',\n",
       " 'tags': ['poem']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afbbee-fab8-4c7f-ad26-094f8c8f4dd9",
   "metadata": {},
   "source": [
    "To filter to the specific LLM invocation, you can use the streamed metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdeee9d9-2625-403a-9253-418a0feeed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "\n",
      "\n",
      "Writing poem...\n",
      "In| shadows| soft|,| they| weave| and| play|,|  \n",
      "|With| whispered| paws|,| they| greet| the| day|.|  \n",
      "|Eyes| like| lantern|s|,| bright| and| keen|,|  \n",
      "|Guard|ians| of| secrets|,| unseen|,| serene|.|  \n",
      "\n",
      "|They| twist| and| stretch| in| sun|lit| beams|,|  \n",
      "|Ch|asing| the| echoes| of| half|-|formed| dreams|.|  \n",
      "|With| p|urring| songs| that| soothe| the| night|,|  \n",
      "|F|eline| spirits|,| pure| delight|.|  \n",
      "\n",
      "|On| windows|ills|,| they| perch| and| stare|,|  \n",
      "|Ad|vent|urers| bold| with| a| graceful| flair|.|  \n",
      "|In| every| leap| and| playful| bound|,|  \n",
      "|The| magic| of| cats|—|where| love| is| found|.|"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "    {\"topic\": \"cats\"},\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    # highlight-next-line\n",
    "    if \"poem\" in metadata.get(\"tags\", []):\n",
    "        print(msg[\"content\"], end=\"|\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
