# serializer version: 1
# name: test_branch_then[memory]
  '''
  graph TD;
  	__start__ --> prepare;
  	finish --> __end__;
  	prepare -.-> tool_two_slow;
  	tool_two_slow --> finish;
  	prepare -.-> tool_two_fast;
  	tool_two_fast --> finish;

  '''
# ---
# name: test_branch_then[memory].1
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	prepare(prepare)
  	tool_two_slow(tool_two_slow)
  	tool_two_fast(tool_two_fast)
  	finish(finish)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> prepare;
  	finish --> __end__;
  	prepare -.-> tool_two_slow;
  	tool_two_slow --> finish;
  	prepare -.-> tool_two_fast;
  	tool_two_fast --> finish;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_branch_then[mongodb]
  '''
  graph TD;
  	__start__ --> prepare;
  	finish --> __end__;
  	prepare -.-> tool_two_slow;
  	tool_two_slow --> finish;
  	prepare -.-> tool_two_fast;
  	tool_two_fast --> finish;

  '''
# ---
# name: test_branch_then[mongodb].1
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	prepare(prepare)
  	tool_two_slow(tool_two_slow)
  	tool_two_fast(tool_two_fast)
  	finish(finish)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> prepare;
  	finish --> __end__;
  	prepare -.-> tool_two_slow;
  	tool_two_slow --> finish;
  	prepare -.-> tool_two_fast;
  	tool_two_fast --> finish;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_conditional_entrypoint_graph
  '{"title": "LangGraphInput"}'
# ---
# name: test_conditional_entrypoint_graph.1
  '{"title": "LangGraphOutput"}'
# ---
# name: test_conditional_entrypoint_graph.2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "left",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "left"
        }
      },
      {
        "id": "right",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "right"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "right",
        "target": "__end__"
      },
      {
        "source": "__start__",
        "target": "left",
        "data": "go-left",
        "conditional": true
      },
      {
        "source": "__start__",
        "target": "right",
        "data": "go-right",
        "conditional": true
      },
      {
        "source": "left",
        "target": "__end__",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_entrypoint_graph.3
  '''
  graph TD;
  	right --> __end__;
  	__start__ -. &nbsp;go-left&nbsp; .-> left;
  	__start__ -. &nbsp;go-right&nbsp; .-> right;
  	left -.-> __end__;

  '''
# ---
# name: test_conditional_entrypoint_graph_state
  '{"properties": {"input": {"default": null, "title": "Input", "type": "string"}, "output": {"default": null, "title": "Output", "type": "string"}, "steps": {"default": null, "items": {"type": "string"}, "title": "Steps", "type": "array"}}, "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_conditional_entrypoint_graph_state.1
  '{"properties": {"input": {"default": null, "title": "Input", "type": "string"}, "output": {"default": null, "title": "Output", "type": "string"}, "steps": {"default": null, "items": {"type": "string"}, "title": "Steps", "type": "array"}}, "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_conditional_entrypoint_graph_state.2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "left",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "left"
        }
      },
      {
        "id": "right",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "right"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "right",
        "target": "__end__"
      },
      {
        "source": "__start__",
        "target": "left",
        "data": "go-left",
        "conditional": true
      },
      {
        "source": "__start__",
        "target": "right",
        "data": "go-right",
        "conditional": true
      },
      {
        "source": "left",
        "target": "__end__",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_entrypoint_graph_state.3
  '''
  graph TD;
  	right --> __end__;
  	__start__ -. &nbsp;go-left&nbsp; .-> left;
  	__start__ -. &nbsp;go-right&nbsp; .-> right;
  	left -.-> __end__;

  '''
# ---
# name: test_conditional_entrypoint_to_multiple_state_graph
  '{"properties": {"locations": {"items": {"type": "string"}, "title": "Locations", "type": "array"}, "results": {"items": {"type": "string"}, "title": "Results", "type": "array"}}, "required": ["locations", "results"], "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_conditional_entrypoint_to_multiple_state_graph.1
  '{"properties": {"locations": {"items": {"type": "string"}, "title": "Locations", "type": "array"}, "results": {"items": {"type": "string"}, "title": "Results", "type": "array"}}, "required": ["locations", "results"], "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_conditional_entrypoint_to_multiple_state_graph.2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "get_weather",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "get_weather"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "get_weather",
        "target": "__end__"
      },
      {
        "source": "__start__",
        "target": "get_weather",
        "conditional": true
      },
      {
        "source": "__start__",
        "target": "__end__",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_entrypoint_to_multiple_state_graph.3
  '''
  graph TD;
  	get_weather --> __end__;
  	__start__ -.-> get_weather;
  	__start__ -.-> __end__;

  '''
# ---
# name: test_conditional_graph[memory]
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnableAssign"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        },
        "metadata": {
          "parents": {},
          "version": 2,
          "variant": "b"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_graph[memory].1
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_graph[memory].2
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	agent(agent)
  	tools(tools<hr/><small><em>parents = {}
  version = 2
  variant = b</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_conditional_graph[memory].3
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": 1,
        "type": "schema",
        "data": "Parallel<agent_outcome>Input"
      },
      {
        "id": 2,
        "type": "schema",
        "data": "Parallel<agent_outcome>Output"
      },
      {
        "id": 3,
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "prompts",
            "prompt",
            "PromptTemplate"
          ],
          "name": "PromptTemplate"
        }
      },
      {
        "id": 4,
        "type": "runnable",
        "data": {
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "name": "FakeStreamingListLLM"
        }
      },
      {
        "id": 5,
        "type": "runnable",
        "data": {
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "name": "agent_parser"
        }
      },
      {
        "id": 6,
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnablePassthrough"
          ],
          "name": "Passthrough"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        },
        "metadata": {
          "parents": {},
          "version": 2,
          "variant": "b"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": 3,
        "target": 4
      },
      {
        "source": 4,
        "target": 5
      },
      {
        "source": 1,
        "target": 3
      },
      {
        "source": 5,
        "target": 2
      },
      {
        "source": 1,
        "target": 6
      },
      {
        "source": 6,
        "target": 2
      },
      {
        "source": "__start__",
        "target": 1
      },
      {
        "source": "tools",
        "target": 1
      },
      {
        "source": 2,
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": 2,
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_graph[memory].4
  '''
  graph TD;
  	PromptTemplate --> FakeStreamingListLLM;
  	FakeStreamingListLLM --> agent_parser;
  	Parallel_agent_outcome_Input --> PromptTemplate;
  	agent_parser --> Parallel_agent_outcome_Output;
  	Parallel_agent_outcome_Input --> Passthrough;
  	Passthrough --> Parallel_agent_outcome_Output;
  	__start__ --> Parallel_agent_outcome_Input;
  	tools --> Parallel_agent_outcome_Input;
  	Parallel_agent_outcome_Output -. &nbsp;continue&nbsp; .-> tools;
  	Parallel_agent_outcome_Output -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_graph[memory].5
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'agent',
      }),
      dict({
        'source': 'tools',
        'target': 'agent',
      }),
      dict({
        'conditional': True,
        'data': 'continue',
        'source': 'agent',
        'target': 'tools',
      }),
      dict({
        'conditional': True,
        'data': 'exit',
        'source': 'agent',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'runnable',
            'RunnableAssign',
          ]),
          'name': 'agent',
        }),
        'id': 'agent',
        'metadata': dict({
          '__interrupt': 'after',
        }),
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tools',
        }),
        'id': 'tools',
        'metadata': dict({
          'parents': dict({
          }),
          'variant': 'b',
          'version': 2,
        }),
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_conditional_graph[memory].6
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	agent(agent<hr/><small><em>__interrupt = after</em></small>)
  	tools(tools<hr/><small><em>parents = {}
  version = 2
  variant = b</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_conditional_graph[mongodb]
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnableAssign"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        },
        "metadata": {
          "parents": {},
          "version": 2,
          "variant": "b"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_graph[mongodb].1
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_graph[mongodb].2
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	agent(agent)
  	tools(tools<hr/><small><em>parents = {}
  version = 2
  variant = b</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_conditional_graph[mongodb].3
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": 1,
        "type": "schema",
        "data": "Parallel<agent_outcome>Input"
      },
      {
        "id": 2,
        "type": "schema",
        "data": "Parallel<agent_outcome>Output"
      },
      {
        "id": 3,
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "prompts",
            "prompt",
            "PromptTemplate"
          ],
          "name": "PromptTemplate"
        }
      },
      {
        "id": 4,
        "type": "runnable",
        "data": {
          "id": [
            "langchain_core",
            "language_models",
            "fake",
            "FakeStreamingListLLM"
          ],
          "name": "FakeStreamingListLLM"
        }
      },
      {
        "id": 5,
        "type": "runnable",
        "data": {
          "id": [
            "langchain_core",
            "runnables",
            "base",
            "RunnableLambda"
          ],
          "name": "agent_parser"
        }
      },
      {
        "id": 6,
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnablePassthrough"
          ],
          "name": "Passthrough"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        },
        "metadata": {
          "parents": {},
          "version": 2,
          "variant": "b"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": 3,
        "target": 4
      },
      {
        "source": 4,
        "target": 5
      },
      {
        "source": 1,
        "target": 3
      },
      {
        "source": 5,
        "target": 2
      },
      {
        "source": 1,
        "target": 6
      },
      {
        "source": 6,
        "target": 2
      },
      {
        "source": "__start__",
        "target": 1
      },
      {
        "source": "tools",
        "target": 1
      },
      {
        "source": 2,
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": 2,
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_graph[mongodb].4
  '''
  graph TD;
  	PromptTemplate --> FakeStreamingListLLM;
  	FakeStreamingListLLM --> agent_parser;
  	Parallel_agent_outcome_Input --> PromptTemplate;
  	agent_parser --> Parallel_agent_outcome_Output;
  	Parallel_agent_outcome_Input --> Passthrough;
  	Passthrough --> Parallel_agent_outcome_Output;
  	__start__ --> Parallel_agent_outcome_Input;
  	tools --> Parallel_agent_outcome_Input;
  	Parallel_agent_outcome_Output -. &nbsp;continue&nbsp; .-> tools;
  	Parallel_agent_outcome_Output -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_graph[mongodb].5
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'agent',
      }),
      dict({
        'source': 'tools',
        'target': 'agent',
      }),
      dict({
        'conditional': True,
        'data': 'continue',
        'source': 'agent',
        'target': 'tools',
      }),
      dict({
        'conditional': True,
        'data': 'exit',
        'source': 'agent',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langchain',
            'schema',
            'runnable',
            'RunnableAssign',
          ]),
          'name': 'agent',
        }),
        'id': 'agent',
        'metadata': dict({
          '__interrupt': 'after',
        }),
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tools',
        }),
        'id': 'tools',
        'metadata': dict({
          'parents': dict({
          }),
          'variant': 'b',
          'version': 2,
        }),
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_conditional_graph[mongodb].6
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	agent(agent<hr/><small><em>__interrupt = after</em></small>)
  	tools(tools<hr/><small><em>parents = {}
  version = 2
  variant = b</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_conditional_state_graph[memory]
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"default": null, "title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_conditional_state_graph[memory].1
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"default": null, "title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_conditional_state_graph[memory].2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnableSequence"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_state_graph[memory].3
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_state_graph[mongodb]
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"default": null, "title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_conditional_state_graph[mongodb].1
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"default": null, "title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_conditional_state_graph[mongodb].2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "langchain",
            "schema",
            "runnable",
            "RunnableSequence"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "tools"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "exit",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_conditional_state_graph[mongodb].3
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;exit&nbsp; .-> __end__;

  '''
# ---
# name: test_conditional_state_graph_with_list_edge_inputs
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "A",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "A"
        }
      },
      {
        "id": "B",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "B"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "A",
        "target": "__end__"
      },
      {
        "source": "B",
        "target": "__end__"
      },
      {
        "source": "__start__",
        "target": "A"
      },
      {
        "source": "__start__",
        "target": "B"
      }
    ]
  }
  '''
# ---
# name: test_conditional_state_graph_with_list_edge_inputs.1
  '''
  graph TD;
  	A --> __end__;
  	B --> __end__;
  	__start__ --> A;
  	__start__ --> B;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge[memory]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query --> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge[mongodb]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query --> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[memory]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[memory].1
  dict({
    'definitions': dict({
      'InnerObject': dict({
        'properties': dict({
          'yo': dict({
            'title': 'Yo',
            'type': 'integer',
          }),
        }),
        'required': list([
          'yo',
        ]),
        'title': 'InnerObject',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'inner': dict({
        '$ref': '#/definitions/InnerObject',
      }),
      'query': dict({
        'title': 'Query',
        'type': 'string',
      }),
    }),
    'required': list([
      'query',
      'inner',
    ]),
    'title': 'Input',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[memory].2
  dict({
    'properties': dict({
      'answer': dict({
        'title': 'Answer',
        'type': 'string',
      }),
      'docs': dict({
        'items': dict({
          'type': 'string',
        }),
        'title': 'Docs',
        'type': 'array',
      }),
    }),
    'required': list([
      'answer',
      'docs',
    ]),
    'title': 'Output',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[mongodb]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[mongodb].1
  dict({
    'definitions': dict({
      'InnerObject': dict({
        'properties': dict({
          'yo': dict({
            'title': 'Yo',
            'type': 'integer',
          }),
        }),
        'required': list([
          'yo',
        ]),
        'title': 'InnerObject',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'inner': dict({
        '$ref': '#/definitions/InnerObject',
      }),
      'query': dict({
        'title': 'Query',
        'type': 'string',
      }),
    }),
    'required': list([
      'query',
      'inner',
    ]),
    'title': 'Input',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic1[mongodb].2
  dict({
    'properties': dict({
      'answer': dict({
        'title': 'Answer',
        'type': 'string',
      }),
      'docs': dict({
        'items': dict({
          'type': 'string',
        }),
        'title': 'Docs',
        'type': 'array',
      }),
    }),
    'required': list([
      'answer',
      'docs',
    ]),
    'title': 'Output',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[memory]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[memory].1
  dict({
    '$defs': dict({
      'InnerObject': dict({
        'properties': dict({
          'yo': dict({
            'title': 'Yo',
            'type': 'integer',
          }),
        }),
        'required': list([
          'yo',
        ]),
        'title': 'InnerObject',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'inner': dict({
        '$ref': '#/$defs/InnerObject',
      }),
      'query': dict({
        'title': 'Query',
        'type': 'string',
      }),
    }),
    'required': list([
      'query',
      'inner',
    ]),
    'title': 'Input',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[memory].2
  dict({
    'properties': dict({
      'answer': dict({
        'title': 'Answer',
        'type': 'string',
      }),
      'docs': dict({
        'items': dict({
          'type': 'string',
        }),
        'title': 'Docs',
        'type': 'array',
      }),
    }),
    'required': list([
      'answer',
      'docs',
    ]),
    'title': 'Output',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[mongodb]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[mongodb].1
  dict({
    '$defs': dict({
      'InnerObject': dict({
        'properties': dict({
          'yo': dict({
            'title': 'Yo',
            'type': 'integer',
          }),
        }),
        'required': list([
          'yo',
        ]),
        'title': 'InnerObject',
        'type': 'object',
      }),
    }),
    'properties': dict({
      'inner': dict({
        '$ref': '#/$defs/InnerObject',
      }),
      'query': dict({
        'title': 'Query',
        'type': 'string',
      }),
    }),
    'required': list([
      'query',
      'inner',
    ]),
    'title': 'Input',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_custom_state_class_pydantic2[mongodb].2
  dict({
    'properties': dict({
      'answer': dict({
        'title': 'Answer',
        'type': 'string',
      }),
      'docs': dict({
        'items': dict({
          'type': 'string',
        }),
        'title': 'Docs',
        'type': 'array',
      }),
    }),
    'required': list([
      'answer',
      'docs',
    ]),
    'title': 'Output',
    'type': 'object',
  })
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_via_branch[memory]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_in_one_fan_out_state_graph_waiting_edge_via_branch[mongodb]
  '''
  graph TD;
  	__start__ --> rewrite_query;
  	analyzer_one --> retriever_one;
  	qa --> __end__;
  	retriever_one --> qa;
  	retriever_two --> qa;
  	rewrite_query --> analyzer_one;
  	rewrite_query -.-> retriever_two;

  '''
# ---
# name: test_message_graph[memory]
  '{"$defs": {"AIMessage": {"additionalProperties": true, "description": "Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ai", "default": "ai", "enum": ["ai"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}}, "required": ["content"], "title": "AIMessage", "type": "object"}, "AIMessageChunk": {"additionalProperties": true, "description": "Message chunk from an AI.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "AIMessageChunk", "default": "AIMessageChunk", "enum": ["AIMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}, "tool_call_chunks": {"default": [], "items": {"$ref": "#/$defs/ToolCallChunk"}, "title": "Tool Call Chunks", "type": "array"}}, "required": ["content"], "title": "AIMessageChunk", "type": "object"}, "ChatMessage": {"additionalProperties": true, "description": "Message that can be assigned an arbitrary speaker (i.e. role).", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "chat", "default": "chat", "enum": ["chat"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessage", "type": "object"}, "ChatMessageChunk": {"additionalProperties": true, "description": "Chat Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ChatMessageChunk", "default": "ChatMessageChunk", "enum": ["ChatMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessageChunk", "type": "object"}, "FunctionMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "function", "default": "function", "enum": ["function"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessage", "type": "object"}, "FunctionMessageChunk": {"additionalProperties": true, "description": "Function Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "FunctionMessageChunk", "default": "FunctionMessageChunk", "enum": ["FunctionMessageChunk"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessageChunk", "type": "object"}, "HumanMessage": {"additionalProperties": true, "description": "Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "human", "default": "human", "enum": ["human"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessage", "type": "object"}, "HumanMessageChunk": {"additionalProperties": true, "description": "Human Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "HumanMessageChunk", "default": "HumanMessageChunk", "enum": ["HumanMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessageChunk", "type": "object"}, "InputTokenDetails": {"description": "Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"cache_creation\\": 200,\\n            \\"cache_read\\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "cache_creation": {"title": "Cache Creation", "type": "integer"}, "cache_read": {"title": "Cache Read", "type": "integer"}}, "title": "InputTokenDetails", "type": "object"}, "InvalidToolCall": {"description": "Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "error": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Error"}, "type": {"const": "invalid_tool_call", "enum": ["invalid_tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "error"], "title": "InvalidToolCall", "type": "object"}, "OutputTokenDetails": {"description": "Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"reasoning\\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "reasoning": {"title": "Reasoning", "type": "integer"}}, "title": "OutputTokenDetails", "type": "object"}, "SystemMessage": {"additionalProperties": true, "description": "Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "system", "default": "system", "enum": ["system"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessage", "type": "object"}, "SystemMessageChunk": {"additionalProperties": true, "description": "System Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "SystemMessageChunk", "default": "SystemMessageChunk", "enum": ["SystemMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessageChunk", "type": "object"}, "ToolCall": {"description": "Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"name\\": \\"foo\\",\\n            \\"args\\": {\\"a\\": 1},\\n            \\"id\\": \\"123\\"\\n        }\\n\\n    This represents a request to call the tool named \\"foo\\" with arguments {\\"a\\": 1}\\n    and an identifier of \\"123\\".", "properties": {"name": {"title": "Name", "type": "string"}, "args": {"title": "Args", "type": "object"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "type": {"const": "tool_call", "enum": ["tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id"], "title": "ToolCall", "type": "object"}, "ToolCallChunk": {"description": "A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\\"foo\\", args=\'{\\"a\\":\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\\"\\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\\"\\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{\\"a\\":1}\', index=0)]", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "index": {"anyOf": [{"type": "integer"}, {"type": "null"}], "title": "Index"}, "type": {"const": "tool_call_chunk", "enum": ["tool_call_chunk"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "index"], "title": "ToolCallChunk", "type": "object"}, "ToolMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \\"stdout\\": \\"From the graph we can see that the correlation between x and y is ...\\",\\n            \\"stderr\\": None,\\n            \\"artifacts\\": {\\"type\\": \\"image\\", \\"base64_data\\": \\"/9j/4gIcSU...\\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\\"stdout\\"],\\n            artifact=tool_output,\\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "tool", "default": "tool", "enum": ["tool"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessage", "type": "object"}, "ToolMessageChunk": {"additionalProperties": true, "description": "Tool Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ToolMessageChunk", "default": "ToolMessageChunk", "enum": ["ToolMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessageChunk", "type": "object"}, "UsageMetadata": {"description": "Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"input_tokens\\": 350,\\n            \\"output_tokens\\": 240,\\n            \\"total_tokens\\": 590,\\n            \\"input_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"cache_creation\\": 200,\\n                \\"cache_read\\": 100,\\n            },\\n            \\"output_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"reasoning\\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.", "properties": {"input_tokens": {"title": "Input Tokens", "type": "integer"}, "output_tokens": {"title": "Output Tokens", "type": "integer"}, "total_tokens": {"title": "Total Tokens", "type": "integer"}, "input_token_details": {"$ref": "#/$defs/InputTokenDetails"}, "output_token_details": {"$ref": "#/$defs/OutputTokenDetails"}}, "required": ["input_tokens", "output_tokens", "total_tokens"], "title": "UsageMetadata", "type": "object"}}, "default": null, "items": {"oneOf": [{"$ref": "#/$defs/AIMessage"}, {"$ref": "#/$defs/HumanMessage"}, {"$ref": "#/$defs/ChatMessage"}, {"$ref": "#/$defs/SystemMessage"}, {"$ref": "#/$defs/FunctionMessage"}, {"$ref": "#/$defs/ToolMessage"}, {"$ref": "#/$defs/AIMessageChunk"}, {"$ref": "#/$defs/HumanMessageChunk"}, {"$ref": "#/$defs/ChatMessageChunk"}, {"$ref": "#/$defs/SystemMessageChunk"}, {"$ref": "#/$defs/FunctionMessageChunk"}, {"$ref": "#/$defs/ToolMessageChunk"}]}, "title": "LangGraphInput", "type": "array"}'
# ---
# name: test_message_graph[memory].1
  '{"$defs": {"AIMessage": {"additionalProperties": true, "description": "Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ai", "default": "ai", "enum": ["ai"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}}, "required": ["content"], "title": "AIMessage", "type": "object"}, "AIMessageChunk": {"additionalProperties": true, "description": "Message chunk from an AI.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "AIMessageChunk", "default": "AIMessageChunk", "enum": ["AIMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}, "tool_call_chunks": {"default": [], "items": {"$ref": "#/$defs/ToolCallChunk"}, "title": "Tool Call Chunks", "type": "array"}}, "required": ["content"], "title": "AIMessageChunk", "type": "object"}, "ChatMessage": {"additionalProperties": true, "description": "Message that can be assigned an arbitrary speaker (i.e. role).", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "chat", "default": "chat", "enum": ["chat"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessage", "type": "object"}, "ChatMessageChunk": {"additionalProperties": true, "description": "Chat Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ChatMessageChunk", "default": "ChatMessageChunk", "enum": ["ChatMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessageChunk", "type": "object"}, "FunctionMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "function", "default": "function", "enum": ["function"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessage", "type": "object"}, "FunctionMessageChunk": {"additionalProperties": true, "description": "Function Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "FunctionMessageChunk", "default": "FunctionMessageChunk", "enum": ["FunctionMessageChunk"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessageChunk", "type": "object"}, "HumanMessage": {"additionalProperties": true, "description": "Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "human", "default": "human", "enum": ["human"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessage", "type": "object"}, "HumanMessageChunk": {"additionalProperties": true, "description": "Human Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "HumanMessageChunk", "default": "HumanMessageChunk", "enum": ["HumanMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessageChunk", "type": "object"}, "InputTokenDetails": {"description": "Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"cache_creation\\": 200,\\n            \\"cache_read\\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "cache_creation": {"title": "Cache Creation", "type": "integer"}, "cache_read": {"title": "Cache Read", "type": "integer"}}, "title": "InputTokenDetails", "type": "object"}, "InvalidToolCall": {"description": "Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "error": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Error"}, "type": {"const": "invalid_tool_call", "enum": ["invalid_tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "error"], "title": "InvalidToolCall", "type": "object"}, "OutputTokenDetails": {"description": "Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"reasoning\\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "reasoning": {"title": "Reasoning", "type": "integer"}}, "title": "OutputTokenDetails", "type": "object"}, "SystemMessage": {"additionalProperties": true, "description": "Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "system", "default": "system", "enum": ["system"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessage", "type": "object"}, "SystemMessageChunk": {"additionalProperties": true, "description": "System Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "SystemMessageChunk", "default": "SystemMessageChunk", "enum": ["SystemMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessageChunk", "type": "object"}, "ToolCall": {"description": "Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"name\\": \\"foo\\",\\n            \\"args\\": {\\"a\\": 1},\\n            \\"id\\": \\"123\\"\\n        }\\n\\n    This represents a request to call the tool named \\"foo\\" with arguments {\\"a\\": 1}\\n    and an identifier of \\"123\\".", "properties": {"name": {"title": "Name", "type": "string"}, "args": {"title": "Args", "type": "object"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "type": {"const": "tool_call", "enum": ["tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id"], "title": "ToolCall", "type": "object"}, "ToolCallChunk": {"description": "A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\\"foo\\", args=\'{\\"a\\":\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\\"\\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\\"\\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{\\"a\\":1}\', index=0)]", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "index": {"anyOf": [{"type": "integer"}, {"type": "null"}], "title": "Index"}, "type": {"const": "tool_call_chunk", "enum": ["tool_call_chunk"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "index"], "title": "ToolCallChunk", "type": "object"}, "ToolMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \\"stdout\\": \\"From the graph we can see that the correlation between x and y is ...\\",\\n            \\"stderr\\": None,\\n            \\"artifacts\\": {\\"type\\": \\"image\\", \\"base64_data\\": \\"/9j/4gIcSU...\\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\\"stdout\\"],\\n            artifact=tool_output,\\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "tool", "default": "tool", "enum": ["tool"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessage", "type": "object"}, "ToolMessageChunk": {"additionalProperties": true, "description": "Tool Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ToolMessageChunk", "default": "ToolMessageChunk", "enum": ["ToolMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessageChunk", "type": "object"}, "UsageMetadata": {"description": "Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"input_tokens\\": 350,\\n            \\"output_tokens\\": 240,\\n            \\"total_tokens\\": 590,\\n            \\"input_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"cache_creation\\": 200,\\n                \\"cache_read\\": 100,\\n            },\\n            \\"output_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"reasoning\\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.", "properties": {"input_tokens": {"title": "Input Tokens", "type": "integer"}, "output_tokens": {"title": "Output Tokens", "type": "integer"}, "total_tokens": {"title": "Total Tokens", "type": "integer"}, "input_token_details": {"$ref": "#/$defs/InputTokenDetails"}, "output_token_details": {"$ref": "#/$defs/OutputTokenDetails"}}, "required": ["input_tokens", "output_tokens", "total_tokens"], "title": "UsageMetadata", "type": "object"}}, "default": null, "items": {"oneOf": [{"$ref": "#/$defs/AIMessage"}, {"$ref": "#/$defs/HumanMessage"}, {"$ref": "#/$defs/ChatMessage"}, {"$ref": "#/$defs/SystemMessage"}, {"$ref": "#/$defs/FunctionMessage"}, {"$ref": "#/$defs/ToolMessage"}, {"$ref": "#/$defs/AIMessageChunk"}, {"$ref": "#/$defs/HumanMessageChunk"}, {"$ref": "#/$defs/ChatMessageChunk"}, {"$ref": "#/$defs/SystemMessageChunk"}, {"$ref": "#/$defs/FunctionMessageChunk"}, {"$ref": "#/$defs/ToolMessageChunk"}]}, "title": "LangGraphOutput", "type": "array"}'
# ---
# name: test_message_graph[memory].2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "tests",
            "test_pregel",
            "FakeFuntionChatModel"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "prebuilt",
            "tool_node",
            "ToolNode"
          ],
          "name": "tools"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "end",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_message_graph[memory].3
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;end&nbsp; .-> __end__;

  '''
# ---
# name: test_message_graph[mongodb]
  '{"$defs": {"AIMessage": {"additionalProperties": true, "description": "Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ai", "default": "ai", "enum": ["ai"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}}, "required": ["content"], "title": "AIMessage", "type": "object"}, "AIMessageChunk": {"additionalProperties": true, "description": "Message chunk from an AI.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "AIMessageChunk", "default": "AIMessageChunk", "enum": ["AIMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}, "tool_call_chunks": {"default": [], "items": {"$ref": "#/$defs/ToolCallChunk"}, "title": "Tool Call Chunks", "type": "array"}}, "required": ["content"], "title": "AIMessageChunk", "type": "object"}, "ChatMessage": {"additionalProperties": true, "description": "Message that can be assigned an arbitrary speaker (i.e. role).", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "chat", "default": "chat", "enum": ["chat"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessage", "type": "object"}, "ChatMessageChunk": {"additionalProperties": true, "description": "Chat Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ChatMessageChunk", "default": "ChatMessageChunk", "enum": ["ChatMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessageChunk", "type": "object"}, "FunctionMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "function", "default": "function", "enum": ["function"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessage", "type": "object"}, "FunctionMessageChunk": {"additionalProperties": true, "description": "Function Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "FunctionMessageChunk", "default": "FunctionMessageChunk", "enum": ["FunctionMessageChunk"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessageChunk", "type": "object"}, "HumanMessage": {"additionalProperties": true, "description": "Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "human", "default": "human", "enum": ["human"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessage", "type": "object"}, "HumanMessageChunk": {"additionalProperties": true, "description": "Human Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "HumanMessageChunk", "default": "HumanMessageChunk", "enum": ["HumanMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessageChunk", "type": "object"}, "InputTokenDetails": {"description": "Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"cache_creation\\": 200,\\n            \\"cache_read\\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "cache_creation": {"title": "Cache Creation", "type": "integer"}, "cache_read": {"title": "Cache Read", "type": "integer"}}, "title": "InputTokenDetails", "type": "object"}, "InvalidToolCall": {"description": "Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "error": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Error"}, "type": {"const": "invalid_tool_call", "enum": ["invalid_tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "error"], "title": "InvalidToolCall", "type": "object"}, "OutputTokenDetails": {"description": "Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"reasoning\\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "reasoning": {"title": "Reasoning", "type": "integer"}}, "title": "OutputTokenDetails", "type": "object"}, "SystemMessage": {"additionalProperties": true, "description": "Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "system", "default": "system", "enum": ["system"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessage", "type": "object"}, "SystemMessageChunk": {"additionalProperties": true, "description": "System Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "SystemMessageChunk", "default": "SystemMessageChunk", "enum": ["SystemMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessageChunk", "type": "object"}, "ToolCall": {"description": "Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"name\\": \\"foo\\",\\n            \\"args\\": {\\"a\\": 1},\\n            \\"id\\": \\"123\\"\\n        }\\n\\n    This represents a request to call the tool named \\"foo\\" with arguments {\\"a\\": 1}\\n    and an identifier of \\"123\\".", "properties": {"name": {"title": "Name", "type": "string"}, "args": {"title": "Args", "type": "object"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "type": {"const": "tool_call", "enum": ["tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id"], "title": "ToolCall", "type": "object"}, "ToolCallChunk": {"description": "A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\\"foo\\", args=\'{\\"a\\":\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\\"\\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\\"\\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{\\"a\\":1}\', index=0)]", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "index": {"anyOf": [{"type": "integer"}, {"type": "null"}], "title": "Index"}, "type": {"const": "tool_call_chunk", "enum": ["tool_call_chunk"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "index"], "title": "ToolCallChunk", "type": "object"}, "ToolMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \\"stdout\\": \\"From the graph we can see that the correlation between x and y is ...\\",\\n            \\"stderr\\": None,\\n            \\"artifacts\\": {\\"type\\": \\"image\\", \\"base64_data\\": \\"/9j/4gIcSU...\\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\\"stdout\\"],\\n            artifact=tool_output,\\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "tool", "default": "tool", "enum": ["tool"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessage", "type": "object"}, "ToolMessageChunk": {"additionalProperties": true, "description": "Tool Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ToolMessageChunk", "default": "ToolMessageChunk", "enum": ["ToolMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessageChunk", "type": "object"}, "UsageMetadata": {"description": "Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"input_tokens\\": 350,\\n            \\"output_tokens\\": 240,\\n            \\"total_tokens\\": 590,\\n            \\"input_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"cache_creation\\": 200,\\n                \\"cache_read\\": 100,\\n            },\\n            \\"output_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"reasoning\\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.", "properties": {"input_tokens": {"title": "Input Tokens", "type": "integer"}, "output_tokens": {"title": "Output Tokens", "type": "integer"}, "total_tokens": {"title": "Total Tokens", "type": "integer"}, "input_token_details": {"$ref": "#/$defs/InputTokenDetails"}, "output_token_details": {"$ref": "#/$defs/OutputTokenDetails"}}, "required": ["input_tokens", "output_tokens", "total_tokens"], "title": "UsageMetadata", "type": "object"}}, "default": null, "items": {"oneOf": [{"$ref": "#/$defs/AIMessage"}, {"$ref": "#/$defs/HumanMessage"}, {"$ref": "#/$defs/ChatMessage"}, {"$ref": "#/$defs/SystemMessage"}, {"$ref": "#/$defs/FunctionMessage"}, {"$ref": "#/$defs/ToolMessage"}, {"$ref": "#/$defs/AIMessageChunk"}, {"$ref": "#/$defs/HumanMessageChunk"}, {"$ref": "#/$defs/ChatMessageChunk"}, {"$ref": "#/$defs/SystemMessageChunk"}, {"$ref": "#/$defs/FunctionMessageChunk"}, {"$ref": "#/$defs/ToolMessageChunk"}]}, "title": "LangGraphInput", "type": "array"}'
# ---
# name: test_message_graph[mongodb].1
  '{"$defs": {"AIMessage": {"additionalProperties": true, "description": "Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ai", "default": "ai", "enum": ["ai"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}}, "required": ["content"], "title": "AIMessage", "type": "object"}, "AIMessageChunk": {"additionalProperties": true, "description": "Message chunk from an AI.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "AIMessageChunk", "default": "AIMessageChunk", "enum": ["AIMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}, "tool_calls": {"default": [], "items": {"$ref": "#/$defs/ToolCall"}, "title": "Tool Calls", "type": "array"}, "invalid_tool_calls": {"default": [], "items": {"$ref": "#/$defs/InvalidToolCall"}, "title": "Invalid Tool Calls", "type": "array"}, "usage_metadata": {"anyOf": [{"$ref": "#/$defs/UsageMetadata"}, {"type": "null"}], "default": null}, "tool_call_chunks": {"default": [], "items": {"$ref": "#/$defs/ToolCallChunk"}, "title": "Tool Call Chunks", "type": "array"}}, "required": ["content"], "title": "AIMessageChunk", "type": "object"}, "ChatMessage": {"additionalProperties": true, "description": "Message that can be assigned an arbitrary speaker (i.e. role).", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "chat", "default": "chat", "enum": ["chat"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessage", "type": "object"}, "ChatMessageChunk": {"additionalProperties": true, "description": "Chat Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ChatMessageChunk", "default": "ChatMessageChunk", "enum": ["ChatMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "role": {"title": "Role", "type": "string"}}, "required": ["content", "role"], "title": "ChatMessageChunk", "type": "object"}, "FunctionMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "function", "default": "function", "enum": ["function"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessage", "type": "object"}, "FunctionMessageChunk": {"additionalProperties": true, "description": "Function Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "FunctionMessageChunk", "default": "FunctionMessageChunk", "enum": ["FunctionMessageChunk"], "title": "Type", "type": "string"}, "name": {"title": "Name", "type": "string"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "name"], "title": "FunctionMessageChunk", "type": "object"}, "HumanMessage": {"additionalProperties": true, "description": "Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "human", "default": "human", "enum": ["human"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessage", "type": "object"}, "HumanMessageChunk": {"additionalProperties": true, "description": "Human Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "HumanMessageChunk", "default": "HumanMessageChunk", "enum": ["HumanMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "example": {"default": false, "title": "Example", "type": "boolean"}}, "required": ["content"], "title": "HumanMessageChunk", "type": "object"}, "InputTokenDetails": {"description": "Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"cache_creation\\": 200,\\n            \\"cache_read\\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "cache_creation": {"title": "Cache Creation", "type": "integer"}, "cache_read": {"title": "Cache Read", "type": "integer"}}, "title": "InputTokenDetails", "type": "object"}, "InvalidToolCall": {"description": "Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "error": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Error"}, "type": {"const": "invalid_tool_call", "enum": ["invalid_tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "error"], "title": "InvalidToolCall", "type": "object"}, "OutputTokenDetails": {"description": "Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"audio\\": 10,\\n            \\"reasoning\\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9", "properties": {"audio": {"title": "Audio", "type": "integer"}, "reasoning": {"title": "Reasoning", "type": "integer"}}, "title": "OutputTokenDetails", "type": "object"}, "SystemMessage": {"additionalProperties": true, "description": "Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\\"You are a helpful assistant! Your name is Bob.\\"\\n            ),\\n            HumanMessage(\\n                content=\\"What is your name?\\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "system", "default": "system", "enum": ["system"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessage", "type": "object"}, "SystemMessageChunk": {"additionalProperties": true, "description": "System Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "SystemMessageChunk", "default": "SystemMessageChunk", "enum": ["SystemMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content"], "title": "SystemMessageChunk", "type": "object"}, "ToolCall": {"description": "Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"name\\": \\"foo\\",\\n            \\"args\\": {\\"a\\": 1},\\n            \\"id\\": \\"123\\"\\n        }\\n\\n    This represents a request to call the tool named \\"foo\\" with arguments {\\"a\\": 1}\\n    and an identifier of \\"123\\".", "properties": {"name": {"title": "Name", "type": "string"}, "args": {"title": "Args", "type": "object"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "type": {"const": "tool_call", "enum": ["tool_call"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id"], "title": "ToolCall", "type": "object"}, "ToolCallChunk": {"description": "A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\\"foo\\", args=\'{\\"a\\":\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\'1}\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\\"\\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\\"\\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\'foo\', args=\'{\\"a\\":1}\', index=0)]", "properties": {"name": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Name"}, "args": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Args"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Id"}, "index": {"anyOf": [{"type": "integer"}, {"type": "null"}], "title": "Index"}, "type": {"const": "tool_call_chunk", "enum": ["tool_call_chunk"], "title": "Type", "type": "string"}}, "required": ["name", "args", "id", "index"], "title": "ToolCallChunk", "type": "object"}, "ToolMessage": {"additionalProperties": true, "description": "Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\'42\', tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \\"stdout\\": \\"From the graph we can see that the correlation between x and y is ...\\",\\n            \\"stderr\\": None,\\n            \\"artifacts\\": {\\"type\\": \\"image\\", \\"base64_data\\": \\"/9j/4gIcSU...\\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\\"stdout\\"],\\n            artifact=tool_output,\\n            tool_call_id=\'call_Jja7J89XsjrOLA5r!MEOW!SL\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "tool", "default": "tool", "enum": ["tool"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessage", "type": "object"}, "ToolMessageChunk": {"additionalProperties": true, "description": "Tool Message chunk.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"const": "ToolMessageChunk", "default": "ToolMessageChunk", "enum": ["ToolMessageChunk"], "title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}, "tool_call_id": {"title": "Tool Call Id", "type": "string"}, "artifact": {"default": null, "title": "Artifact"}, "status": {"default": "success", "enum": ["success", "error"], "title": "Status", "type": "string"}}, "required": ["content", "tool_call_id"], "title": "ToolMessageChunk", "type": "object"}, "UsageMetadata": {"description": "Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \\"input_tokens\\": 350,\\n            \\"output_tokens\\": 240,\\n            \\"total_tokens\\": 590,\\n            \\"input_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"cache_creation\\": 200,\\n                \\"cache_read\\": 100,\\n            },\\n            \\"output_token_details\\": {\\n                \\"audio\\": 10,\\n                \\"reasoning\\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.", "properties": {"input_tokens": {"title": "Input Tokens", "type": "integer"}, "output_tokens": {"title": "Output Tokens", "type": "integer"}, "total_tokens": {"title": "Total Tokens", "type": "integer"}, "input_token_details": {"$ref": "#/$defs/InputTokenDetails"}, "output_token_details": {"$ref": "#/$defs/OutputTokenDetails"}}, "required": ["input_tokens", "output_tokens", "total_tokens"], "title": "UsageMetadata", "type": "object"}}, "default": null, "items": {"oneOf": [{"$ref": "#/$defs/AIMessage"}, {"$ref": "#/$defs/HumanMessage"}, {"$ref": "#/$defs/ChatMessage"}, {"$ref": "#/$defs/SystemMessage"}, {"$ref": "#/$defs/FunctionMessage"}, {"$ref": "#/$defs/ToolMessage"}, {"$ref": "#/$defs/AIMessageChunk"}, {"$ref": "#/$defs/HumanMessageChunk"}, {"$ref": "#/$defs/ChatMessageChunk"}, {"$ref": "#/$defs/SystemMessageChunk"}, {"$ref": "#/$defs/FunctionMessageChunk"}, {"$ref": "#/$defs/ToolMessageChunk"}]}, "title": "LangGraphOutput", "type": "array"}'
# ---
# name: test_message_graph[mongodb].2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "tests",
            "test_pregel",
            "FakeFuntionChatModel"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "prebuilt",
            "tool_node",
            "ToolNode"
          ],
          "name": "tools"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "data": "continue",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "data": "end",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_message_graph[mongodb].3
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -. &nbsp;continue&nbsp; .-> tools;
  	agent -. &nbsp;end&nbsp; .-> __end__;

  '''
# ---
# name: test_multiple_sinks_subgraphs
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	uno(uno)
  	dos(dos)
  	subgraph_one(one)
  	subgraph_two(two)
  	subgraph_three(three)
  	__start__ --> uno;
  	uno -.-> dos;
  	uno -.-> subgraph_one;
  	subgraph subgraph
  	subgraph_one -.-> subgraph_two;
  	subgraph_one -.-> subgraph_three;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_nested_graph
  '''
  graph TD;
  	__start__ --> inner;
  	inner --> side;
  	side --> __end__;

  '''
# ---
# name: test_nested_graph.1
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	inner(inner)
  	side(side)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> inner;
  	inner --> side;
  	side --> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_nested_graph_xray
  dict({
    'edges': list([
      dict({
        'conditional': True,
        'source': 'tool_two:__start__',
        'target': 'tool_two:tool_two_slow',
      }),
      dict({
        'source': 'tool_two:tool_two_slow',
        'target': 'tool_two:__end__',
      }),
      dict({
        'conditional': True,
        'source': 'tool_two:__start__',
        'target': 'tool_two:tool_two_fast',
      }),
      dict({
        'source': 'tool_two:tool_two_fast',
        'target': 'tool_two:__end__',
      }),
      dict({
        'conditional': True,
        'source': '__start__',
        'target': 'tool_one',
      }),
      dict({
        'source': 'tool_one',
        'target': '__end__',
      }),
      dict({
        'conditional': True,
        'source': '__start__',
        'target': 'tool_two:__start__',
      }),
      dict({
        'source': 'tool_two:__end__',
        'target': '__end__',
      }),
      dict({
        'conditional': True,
        'source': '__start__',
        'target': 'tool_three',
      }),
      dict({
        'source': 'tool_three',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tool_one',
        }),
        'id': 'tool_one',
        'type': 'runnable',
      }),
      dict({
        'data': 'tool_two:__start__',
        'id': 'tool_two:__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tool_two:tool_two_slow',
        }),
        'id': 'tool_two:tool_two_slow',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tool_two:tool_two_fast',
        }),
        'id': 'tool_two:tool_two_fast',
        'type': 'runnable',
      }),
      dict({
        'data': 'tool_two:__end__',
        'id': 'tool_two:__end__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'tool_three',
        }),
        'id': 'tool_three',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_nested_graph_xray.1
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	tool_one(tool_one)
  	tool_two___start__(<p>__start__</p>)
  	tool_two_tool_two_slow(tool_two_slow)
  	tool_two_tool_two_fast(tool_two_fast)
  	tool_two___end__(<p>__end__</p>)
  	tool_three(tool_three)
  	__end__([<p>__end__</p>]):::last
  	__start__ -.-> tool_one;
  	tool_one --> __end__;
  	__start__ -.-> tool_two___start__;
  	tool_two___end__ --> __end__;
  	__start__ -.-> tool_three;
  	tool_three --> __end__;
  	subgraph tool_two
  	tool_two___start__ -.-> tool_two_tool_two_slow;
  	tool_two_tool_two_slow --> tool_two___end__;
  	tool_two___start__ -.-> tool_two_tool_two_fast;
  	tool_two_tool_two_fast --> tool_two___end__;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_prebuilt_tool_chat
  '{"$defs": {"BaseMessage": {"additionalProperties": true, "description": "Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "type"], "title": "BaseMessage", "type": "object"}}, "properties": {"messages": {"items": {"$ref": "#/$defs/BaseMessage"}, "title": "Messages", "type": "array"}}, "required": ["messages"], "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_prebuilt_tool_chat.1
  '{"$defs": {"BaseMessage": {"additionalProperties": true, "description": "Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.", "properties": {"content": {"anyOf": [{"type": "string"}, {"items": {"anyOf": [{"type": "string"}, {"type": "object"}]}, "type": "array"}], "title": "Content"}, "additional_kwargs": {"title": "Additional Kwargs", "type": "object"}, "response_metadata": {"title": "Response Metadata", "type": "object"}, "type": {"title": "Type", "type": "string"}, "name": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Name"}, "id": {"anyOf": [{"type": "string"}, {"type": "null"}], "default": null, "title": "Id"}}, "required": ["content", "type"], "title": "BaseMessage", "type": "object"}}, "properties": {"messages": {"items": {"$ref": "#/$defs/BaseMessage"}, "title": "Messages", "type": "array"}}, "required": ["messages"], "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_prebuilt_tool_chat.2
  '''
  {
    "nodes": [
      {
        "id": "__start__",
        "type": "schema",
        "data": "__start__"
      },
      {
        "id": "agent",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "utils",
            "runnable",
            "RunnableCallable"
          ],
          "name": "agent"
        }
      },
      {
        "id": "tools",
        "type": "runnable",
        "data": {
          "id": [
            "langgraph",
            "prebuilt",
            "tool_node",
            "ToolNode"
          ],
          "name": "tools"
        }
      },
      {
        "id": "__end__",
        "type": "schema",
        "data": "__end__"
      }
    ],
    "edges": [
      {
        "source": "__start__",
        "target": "agent"
      },
      {
        "source": "tools",
        "target": "agent"
      },
      {
        "source": "agent",
        "target": "tools",
        "conditional": true
      },
      {
        "source": "agent",
        "target": "__end__",
        "conditional": true
      }
    ]
  }
  '''
# ---
# name: test_prebuilt_tool_chat.3
  '''
  graph TD;
  	__start__ --> agent;
  	tools --> agent;
  	agent -.-> tools;
  	agent -.-> __end__;

  '''
# ---
# name: test_repeat_condition
  '''
  graph TD;
  	__start__ --> Researcher;
  	Researcher -. &nbsp;continue&nbsp; .-> Chart_Generator;
  	Researcher -. &nbsp;call_tool&nbsp; .-> Call_Tool;
  	Researcher -. &nbsp;end&nbsp; .-> __end__;
  	Chart_Generator -. &nbsp;continue&nbsp; .-> Researcher;
  	Chart_Generator -. &nbsp;call_tool&nbsp; .-> Call_Tool;
  	Chart_Generator -. &nbsp;end&nbsp; .-> __end__;
  	Call_Tool -.-> Researcher;
  	Call_Tool -.-> Chart_Generator;
  	Researcher -. &nbsp;redo&nbsp; .-> Researcher;

  '''
# ---
# name: test_simple_multi_edge
  '''
  graph TD;
  	__start__ --> up;
  	down --> __end__;
  	side --> down;
  	up --> down;
  	up --> other;
  	up --> side;

  '''
# ---
# name: test_start_branch_then[memory]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	tool_two_slow(tool_two_slow)
  	tool_two_fast(tool_two_fast)
  	__end__([<p>__end__</p>]):::last
  	__start__ -.-> tool_two_slow;
  	tool_two_slow --> __end__;
  	__start__ -.-> tool_two_fast;
  	tool_two_fast --> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_start_branch_then[mongodb]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	tool_two_slow(tool_two_slow)
  	tool_two_fast(tool_two_fast)
  	__end__([<p>__end__</p>]):::last
  	__start__ -.-> tool_two_slow;
  	tool_two_slow --> __end__;
  	__start__ -.-> tool_two_fast;
  	tool_two_fast --> __end__;
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_state_graph_w_config_inherited_state_keys
  '{"$defs": {"Configurable": {"properties": {"tools": {"default": null, "items": {"type": "string"}, "title": "Tools", "type": "array"}}, "title": "Configurable", "type": "object"}}, "properties": {"configurable": {"$ref": "#/$defs/Configurable", "default": null}}, "title": "LangGraphConfig", "type": "object"}'
# ---
# name: test_state_graph_w_config_inherited_state_keys.1
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "required": ["input"], "title": "LangGraphInput", "type": "object"}'
# ---
# name: test_state_graph_w_config_inherited_state_keys.2
  '{"$defs": {"AgentAction": {"description": "Represents a request to execute an action by an agent.\\n\\nThe action consists of the name of the tool to execute and the input to pass\\nto the tool. The log is used to pass along extra information about the action.", "properties": {"tool": {"title": "Tool", "type": "string"}, "tool_input": {"anyOf": [{"type": "string"}, {"type": "object"}], "title": "Tool Input"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentAction", "default": "AgentAction", "enum": ["AgentAction"], "title": "Type", "type": "string"}}, "required": ["tool", "tool_input", "log"], "title": "AgentAction", "type": "object"}, "AgentFinish": {"description": "Final return value of an ActionAgent.\\n\\nAgents return an AgentFinish when they have reached a stopping condition.", "properties": {"return_values": {"title": "Return Values", "type": "object"}, "log": {"title": "Log", "type": "string"}, "type": {"const": "AgentFinish", "default": "AgentFinish", "enum": ["AgentFinish"], "title": "Type", "type": "string"}}, "required": ["return_values", "log"], "title": "AgentFinish", "type": "object"}}, "properties": {"input": {"title": "Input", "type": "string"}, "agent_outcome": {"anyOf": [{"$ref": "#/$defs/AgentAction"}, {"$ref": "#/$defs/AgentFinish"}, {"type": "null"}], "default": null, "title": "Agent Outcome"}, "intermediate_steps": {"default": null, "items": {"maxItems": 2, "minItems": 2, "prefixItems": [{"$ref": "#/$defs/AgentAction"}, {"type": "string"}], "type": "array"}, "title": "Intermediate Steps", "type": "array"}}, "required": ["input"], "title": "LangGraphOutput", "type": "object"}'
# ---
# name: test_weather_subgraph[memory]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	router_node(router_node)
  	normal_llm_node(normal_llm_node)
  	weather_graph_model_node(model_node)
  	weather_graph_weather_node(weather_node<hr/><small><em>__interrupt = before</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> router_node;
  	normal_llm_node --> __end__;
  	weather_graph_weather_node --> __end__;
  	router_node -.-> normal_llm_node;
  	router_node -.-> weather_graph_model_node;
  	router_node -.-> __end__;
  	subgraph weather_graph
  	weather_graph_model_node --> weather_graph_weather_node;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_weather_subgraph[mongodb]
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	router_node(router_node)
  	normal_llm_node(normal_llm_node)
  	weather_graph_model_node(model_node)
  	weather_graph_weather_node(weather_node<hr/><small><em>__interrupt = before</em></small>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> router_node;
  	normal_llm_node --> __end__;
  	weather_graph_weather_node --> __end__;
  	router_node -.-> normal_llm_node;
  	router_node -.-> weather_graph_model_node;
  	router_node -.-> __end__;
  	subgraph weather_graph
  	weather_graph_model_node --> weather_graph_weather_node;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_xray_bool
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	gp_one(gp_one)
  	gp_two___start__(<p>__start__</p>)
  	gp_two_p_one(p_one)
  	gp_two_p_two___start__(<p>__start__</p>)
  	gp_two_p_two_c_one(c_one)
  	gp_two_p_two_c_two(c_two)
  	gp_two_p_two___end__(<p>__end__</p>)
  	gp_two___end__(<p>__end__</p>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> gp_one;
  	gp_two___end__ --> gp_one;
  	gp_one -. &nbsp;0&nbsp; .-> gp_two___start__;
  	gp_one -. &nbsp;1&nbsp; .-> __end__;
  	subgraph gp_two
  	gp_two___start__ --> gp_two_p_one;
  	gp_two_p_two___end__ --> gp_two_p_one;
  	gp_two_p_one -. &nbsp;0&nbsp; .-> gp_two_p_two___start__;
  	gp_two_p_one -. &nbsp;1&nbsp; .-> gp_two___end__;
  	subgraph p_two
  	gp_two_p_two___start__ --> gp_two_p_two_c_one;
  	gp_two_p_two_c_two --> gp_two_p_two_c_one;
  	gp_two_p_two_c_one -. &nbsp;0&nbsp; .-> gp_two_p_two_c_two;
  	gp_two_p_two_c_one -. &nbsp;1&nbsp; .-> gp_two_p_two___end__;
  	end
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_xray_issue
  '''
  %%{init: {'flowchart': {'curve': 'linear'}}}%%
  graph TD;
  	__start__([<p>__start__</p>]):::first
  	p_one(p_one)
  	p_two___start__(<p>__start__</p>)
  	p_two_c_one(c_one)
  	p_two_c_two(c_two)
  	p_two___end__(<p>__end__</p>)
  	__end__([<p>__end__</p>]):::last
  	__start__ --> p_one;
  	p_two___end__ --> p_one;
  	p_one -. &nbsp;0&nbsp; .-> p_two___start__;
  	p_one -. &nbsp;1&nbsp; .-> __end__;
  	subgraph p_two
  	p_two___start__ --> p_two_c_one;
  	p_two_c_two --> p_two_c_one;
  	p_two_c_one -. &nbsp;0&nbsp; .-> p_two_c_two;
  	p_two_c_one -. &nbsp;1&nbsp; .-> p_two___end__;
  	end
  	classDef default fill:#f2f0ff,line-height:1.2
  	classDef first fill-opacity:0
  	classDef last fill:#bfb6fc

  '''
# ---
# name: test_xray_lance
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'ask_question',
      }),
      dict({
        'source': 'ask_question',
        'target': 'answer_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': 'ask_question',
      }),
      dict({
        'conditional': True,
        'source': 'answer_question',
        'target': '__end__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'ask_question',
        }),
        'id': 'ask_question',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'answer_question',
        }),
        'id': 'answer_question',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_xray_lance.1
  dict({
    'edges': list([
      dict({
        'source': '__start__',
        'target': 'generate_analysts',
      }),
      dict({
        'source': 'conduct_interview',
        'target': 'generate_sections',
      }),
      dict({
        'source': 'generate_sections',
        'target': '__end__',
      }),
      dict({
        'conditional': True,
        'source': 'generate_analysts',
        'target': 'conduct_interview',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'generate_analysts',
        }),
        'id': 'generate_analysts',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'graph',
            'state',
            'CompiledStateGraph',
          ]),
          'name': 'conduct_interview',
        }),
        'id': 'conduct_interview',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'generate_sections',
        }),
        'id': 'generate_sections',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
# name: test_xray_lance.2
  dict({
    'edges': list([
      dict({
        'source': 'conduct_interview:__start__',
        'target': 'conduct_interview:ask_question',
      }),
      dict({
        'source': 'conduct_interview:ask_question',
        'target': 'conduct_interview:answer_question',
      }),
      dict({
        'conditional': True,
        'source': 'conduct_interview:answer_question',
        'target': 'conduct_interview:ask_question',
      }),
      dict({
        'conditional': True,
        'source': 'conduct_interview:answer_question',
        'target': 'conduct_interview:__end__',
      }),
      dict({
        'source': '__start__',
        'target': 'generate_analysts',
      }),
      dict({
        'source': 'conduct_interview:__end__',
        'target': 'generate_sections',
      }),
      dict({
        'source': 'generate_sections',
        'target': '__end__',
      }),
      dict({
        'conditional': True,
        'source': 'generate_analysts',
        'target': 'conduct_interview:__start__',
      }),
    ]),
    'nodes': list([
      dict({
        'data': '__start__',
        'id': '__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'generate_analysts',
        }),
        'id': 'generate_analysts',
        'type': 'runnable',
      }),
      dict({
        'data': 'conduct_interview:__start__',
        'id': 'conduct_interview:__start__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'conduct_interview:ask_question',
        }),
        'id': 'conduct_interview:ask_question',
        'type': 'runnable',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'conduct_interview:answer_question',
        }),
        'id': 'conduct_interview:answer_question',
        'type': 'runnable',
      }),
      dict({
        'data': 'conduct_interview:__end__',
        'id': 'conduct_interview:__end__',
        'type': 'schema',
      }),
      dict({
        'data': dict({
          'id': list([
            'langgraph',
            'utils',
            'runnable',
            'RunnableCallable',
          ]),
          'name': 'generate_sections',
        }),
        'id': 'generate_sections',
        'type': 'runnable',
      }),
      dict({
        'data': '__end__',
        'id': '__end__',
        'type': 'schema',
      }),
    ]),
  })
# ---
